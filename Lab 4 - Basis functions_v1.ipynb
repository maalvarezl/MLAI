{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basis Functions\n",
    "\n",
    "### 20th October 2015 Neil Lawrence\n",
    "\n",
    "We've now seen how we may perform linear regression. Now, we are going to consider how we can perform *non-linear* regression. However, before we get into the details of how to do that we first need to consider in what ways the regression can be non-linear. \n",
    "\n",
    "Multivariate linear regression allows us to build models that take many features into account when making our prediction. In this session we are going to introduce *basis functions*. The term seems complicated, but they are actually based on rather a simple idea. If we are doing a multivariate linear regression, we get extra features that *might* help us predict our required response variable (or target value), $y$. But what if we only have one input value? We can actually artificially generate more input values with basis functions.\n",
    "\n",
    "## Non-linear in the Inputs\n",
    "\n",
    "When we refer to non-linear regression, we are normally referring to whether the regression is non-linear in the input space, or non-linear in the *covariates*. The covariates are the observations that move with the target (or *response*) variable. In our notation we have been using $\\mathbf{x}_i$ to represent a vector of the covariates associated with the $i$th observation. The coresponding response variable is $y_i$. If a model is non-linear in the inputs, it means that there is a non-linear function between the inputs and the response variable. Linear functions are functions that only involve multiplication and addition, in other words they can be represented through *linear algebra*. Linear regression involves assuming that a function takes the form\n",
    "$$\n",
    "f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x}\n",
    "$$\n",
    "where $\\mathbf{w}$ are our regression weights. A very easy way to make the linear regression non-linear is to introduce non-linear functions. When we are introducing non-linear regression these functions are known as *basis functions*.\n",
    "\n",
    "### Basis Functions\n",
    "\n",
    "Here's the idea, instead of working directly on the original input space, $\\mathbf{x}$, we build models in a new space, $\\boldsymbol{\\phi}(\\mathbf{x})$ where $\\boldsymbol{\\phi}(\\cdot)$ is a *vector valued* function that is defined on the space $\\mathbf{x}$. \n",
    "\n",
    "Remember, that a vector valued function is just a vector that contains functions instead of values. Here's an example for a one dimensional input space, $x$, being projected to a *quadratic* basis. First we consider each basis function in turn, we can think of the elements of our vector as being indexed so that we have\n",
    "\\begin{align*}\n",
    "\\phi_1(x) = 1, \\\\\n",
    "\\phi_2(x) = x, \\\\\n",
    "\\phi_3(x) = x^2.\n",
    "\\end{align*}\n",
    "Now we can consider them together by placing them in a vector,\n",
    "$$\n",
    "\\boldsymbol{\\phi}(x) = \\begin{bmatrix} 1\\\\ x \\\\ x^2\\end{bmatrix}.\n",
    "$$\n",
    "This is the idea of the vector valued function, we have simply collected the different functions together in the same vector making them notationally easier to deal with in our mathematics. \n",
    "\n",
    "When we consider the vector valued function for each data point, then we place all the data into a matrix. The result is a matrix valued function,\n",
    "$$\n",
    "\\boldsymbol{\\Phi}(\\mathbf{x}) = \n",
    "\\begin{bmatrix} 1 & x_1 & x_1^2 \\\\\n",
    "1 & x_2 & x_2^2\\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "1 & x_n & x_n^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where we are still in the one dimensional input setting so $\\mathbf{x}$ here represents a vector of our inputs with $n$ elements. \n",
    "\n",
    "Let's try constructing such a matrix for a set of inputs. First of all, we create a function that returns the matrix valued function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # import numpy for the arrays.\n",
    "\n",
    "def quadratic(x):\n",
    "    \"\"\"Take in a vector of input values and return the design matrix associated \n",
    "    with the basis functions.\"\"\"\n",
    "    return np.hstack([np.ones((n, 1)), x, x**2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in an $n\\times 1$  dimensional vector and returns an $n\\times 3$ dimensional *design matrix* containing the basis functions. We can plot those basis functions against there input as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure plots appear in the notebook.\n",
    "%matplotlib inline \n",
    "import pylab as plt\n",
    "\n",
    "# first let's generate some inputs\n",
    "n = 100\n",
    "x = np.zeros((n, 1))  # create a data set of zeros\n",
    "x[:, 0] = np.linspace(-1, 1, n) # fill it with values between -1 and 1\n",
    "\n",
    "Phi = quadratic(x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,4))\n",
    "ax.set_ylim([-1.2, 1.2]) # set y limits to ensure basis functions show.\n",
    "ax.plot(x[:,0], Phi[:, 0], 'r-', label = '$\\phi=1$')\n",
    "ax.plot(x[:,0], Phi[:, 1], 'g-', label = '$\\phi = x$')\n",
    "ax.plot(x[:,0], Phi[:, 2], 'b-', label = '$\\phi = x^2$')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_title('Quadratic Basis Functions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual function we observe is then made up of a sum of these functions. This is the reason for the name basis. The term *basis* means 'the underlying support or foundation for an idea, argument, or process', and in this context they form the underlying support for our prediction function. Our prediction function can only be composed of a weighted linear sum of our basis functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Basis\n",
    "\n",
    "Before we look at the different types of basis functions available, we need to run the following cell of code that will be used in the rest of the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial code: it uses pods.notebook.display_prediction, but with a minor modification to \n",
    "# allow the use of ipywidgets\n",
    "from ipywidgets import *\n",
    "def display_prediction(basis, num_basis=4, wlim=(-1.,1.), fig=None, ax=None, xlim=None, ylim=None, num_points=1000, offset=0.0, **kwargs):\n",
    "    \"\"\"Interactive widget for displaying a prediction function based on summing separate basis functions.\n",
    "    :param basis: a function handle that calls the basis functions.\n",
    "    :type basis: function handle.\n",
    "    :param xlim: limits of the x axis to use.\n",
    "    :param ylim: limits of the y axis to use.\n",
    "    :param wlim: limits for the basis function weights.\"\"\"\n",
    "\n",
    "    #import numpy as np\n",
    "    #import pylab as plt\n",
    "\n",
    "    if fig is not None:\n",
    "        if ax is None:\n",
    "            ax = fig.gca()\n",
    "\n",
    "    if xlim is None:\n",
    "        if ax is not None:\n",
    "            xlim = ax.get_xlim()\n",
    "        else:\n",
    "            xlim = (-2., 2.)\n",
    "    if ylim is None:\n",
    "        if ax is not None:\n",
    "            ylim = ax.get_ylim()\n",
    "        else:\n",
    "            ylim = (-1., 1.)\n",
    "\n",
    "    # initialise X and set up W arguments.\n",
    "    x = np.zeros((num_points, 1))\n",
    "    x[:, 0] = np.linspace(xlim[0], xlim[1], num_points)\n",
    "    param_args = {}\n",
    "    for i in range(num_basis):\n",
    "        lim = list(wlim)\n",
    "        if i ==0:\n",
    "            lim[0] += offset\n",
    "            lim[1] += offset\n",
    "        param_args['w_' + str(i)] = tuple(lim)\n",
    "\n",
    "    # helper function for making basis prediction.\n",
    "    def predict_basis(w, basis, x, num_basis, **kwargs):\n",
    "        Phi = basis(x, num_basis, **kwargs)\n",
    "        f = np.dot(Phi, w)\n",
    "        return f, Phi\n",
    "    \n",
    "    if type(basis) is dict:\n",
    "        use_basis = basis[list(basis.keys())[0]]\n",
    "    else:\n",
    "        use_basis = basis\n",
    "    f, Phi = predict_basis(np.zeros((num_basis, 1)),\n",
    "                           use_basis, x, num_basis,\n",
    "                           **kwargs)\n",
    "    if fig is None:\n",
    "        fig, ax=plt.subplots(figsize=(12,4))\n",
    "        ax.set_ylim(ylim)\n",
    "        ax.set_xlim(xlim)\n",
    "\n",
    "    predline = ax.plot(x, f, linewidth=2)[0]\n",
    "    basislines = []\n",
    "    for i in range(num_basis):\n",
    "        basislines.append(ax.plot(x, Phi[:, i], 'r')[0])\n",
    "\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_xlim(xlim)\n",
    "\n",
    "    def generate_function(basis, num_basis, predline, basislines, basis_args, display_basis, offset, **kwargs):\n",
    "        w = np.zeros((num_basis, 1))\n",
    "        for i in range(num_basis):\n",
    "            w[i] = kwargs['w_'+ str(i)]\n",
    "        f, Phi = predict_basis(w, basis, x, num_basis, **basis_args)\n",
    "        predline.set_xdata(x[:, 0])\n",
    "        predline.set_ydata(f)\n",
    "        for i in range(num_basis):\n",
    "            basislines[i].set_xdata(x[:, 0])\n",
    "            basislines[i].set_ydata(Phi[:, i])\n",
    "\n",
    "        if display_basis:\n",
    "            for i in range(num_basis):\n",
    "                basislines[i].set_alpha(1) # make visible\n",
    "        else:\n",
    "            for i in range(num_basis):\n",
    "                basislines[i].set_alpha(0) \n",
    "        display(fig)\n",
    "    if type(basis) is not dict:\n",
    "        basis = fixed(basis)\n",
    "\n",
    "    plt.close(fig)\n",
    "    interact(generate_function, \n",
    "             basis=basis,\n",
    "             num_basis=fixed(num_basis),\n",
    "             predline=fixed(predline),\n",
    "             basislines=fixed(basislines),\n",
    "             basis_args=fixed(kwargs),\n",
    "             offset = fixed(offset),\n",
    "             display_basis = False,\n",
    "             **param_args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our choice of basis can be made based on what our beliefs about what is appropriate for the data. For example, the polynomial basis extends the quadratic basis to arbitrary degree, so we might define the $j$th basis function associated with the model as\n",
    "$$\n",
    "\\phi_j(x_i) = x_i^j\n",
    "$$\n",
    "which can be implemented as a function in code as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial(x, num_basis=4, data_limits=[-1., 1.]):\n",
    "    Phi = np.zeros((x.shape[0], num_basis))\n",
    "    for i in range(num_basis):\n",
    "        Phi[:, i:i+1] = x**i\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To aid in understanding how a basis works, we've provided you with a small interactive tool for exploring this polynomial basis. The tool can be summoned with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_prediction(basis=polynomial, num_basis=4, ylim=[-3.,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try moving the sliders around to change the weight of each basis function. Click the control box `display_basis` to show the underlying basis functions (in red). The prediction function is shown in a thick blue line. *Warning* the sliders aren't presented quite in the correct order. `w_0` is associated with the bias, `w_1` is the linear term, `w_2` the quadratic and here (because we have four basis functions) we have `w_3` for the *cubic* term. So the subscript of the weight parameter is always associated with the corresponding polynomial's degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Try increasing the number of basis functions (thereby increasing the *degree* of the resulting polynomial). Describe what you see as you increase number of basis up to 10. Is it easy to change the function in intuitive ways?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 Answer\n",
    "\n",
    "Write your answer to the question in this box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radial Basis Functions\n",
    "\n",
    "Another type of basis is sometimes known as a 'radial basis' because the effect basis functions are constructed on 'centres' and the effect of each basis function decreases as the radial distance from each centre increases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s radial mlai.py\n",
    "def radial(x, num_basis=4, data_limits=[-1., 1.]):\n",
    "    \"Radial basis constructed using exponentiated quadratic form.\"\n",
    "    if num_basis>1:\n",
    "        centres=np.linspace(data_limits[0], data_limits[1], num_basis)\n",
    "        width = (centres[1]-centres[0])/2.\n",
    "    else:\n",
    "        centres = np.asarray([data_limits[0]/2. + data_limits[1]/2.])\n",
    "        width = (data_limits[1]-data_limits[0])/2.\n",
    "    \n",
    "    Phi = np.zeros((x.shape[0], num_basis))\n",
    "    for i in range(num_basis):\n",
    "        Phi[:, i:i+1] = np.exp(-0.5*((x-centres[i])/width)**2)\n",
    "    return Phi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_prediction(basis=radial, num_basis=4, ylim=[-2., 2.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier Basis\n",
    "\n",
    "Fourier noticed that any *stationary* function could be converted to a sum of sines and cosines. A Fourier basis is a linear weighted sum of these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s fourier mlai.py\n",
    "def fourier(x, num_basis=4, data_limits=[-2., 2.]):\n",
    "    \"Fourier basis\"\n",
    "    tau = 2*np.pi\n",
    "    span = float(data_limits[1]-data_limits[0])\n",
    "    Phi = np.zeros((x.shape[0], num_basis))\n",
    "    for i in range(num_basis):\n",
    "        count = float((i+1)//2)\n",
    "        frequency = count/span\n",
    "        if i % 2:\n",
    "            Phi[:, i:i+1] = np.sin(tau*frequency*x)\n",
    "        else:\n",
    "            Phi[:, i:i+1] = np.cos(tau*frequency*x)\n",
    "    return Phi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, basis functions with an *odd* index are sine and basis functions with an *even* index are cosine. The first basis function (index 0, so cosine) has a frequency of 0 and then frequencies increase to 1, 2, 3, 4 etc every time a sine and cosine are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_prediction(basis=fourier, num_basis=4, ylim=[-1.5, 1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s relu mlai.py\n",
    "def relu(x, num_basis=4, data_limits=[-1., 1.], gain=None):\n",
    "    \"Rectified linear units basis\"\n",
    "    if num_basis>2:\n",
    "        centres=np.linspace(data_limits[0], data_limits[1], num_basis)\n",
    "    else:\n",
    "        centres = np.asarray([data_limits[0]/2. + data_limits[1]/2.])\n",
    "    if gain is None:\n",
    "        gain = np.ones(num_basis-1)\n",
    "    Phi = np.zeros((x.shape[0], num_basis))\n",
    "    # Create the bias\n",
    "    Phi[:, 0] = 1.0\n",
    "    for i in range(1, num_basis):\n",
    "        Phi[:, i:i+1] = (gain[i-1]*x>centres[i-1])*(x-centres[i-1])\n",
    "    return Phi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_prediction(basis=relu, num_basis=4, ylim=[-2., 2.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting to Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to consider how these basis functions can be adjusted to fit to a particular data set. We will return to the olympic marathon data from last time. First we will scale the output of the data to be zero mean and variance 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "data = pods.datasets.olympic_marathon_men()\n",
    "y = data['Y']\n",
    "x = data['X']\n",
    "y -= y.mean()\n",
    "y /= y.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Now we are going to redefine our polynomial basis. Have a careful look at the operations we perform on `x` to create `z`. We use `z` in the polynomial computation. What are we doing to the inputs? Why do you think we are changing `x` in this manner?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 Answer\n",
    "\n",
    "Write your answer to the question in this box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s polynomial mlai.py\n",
    "def polynomial(x, num_basis=4, data_limits=[-1., 1.]):\n",
    "    \"Polynomial basis\"\n",
    "    centre = data_limits[0]/2. + data_limits[1]/2.\n",
    "    span = data_limits[1] - data_limits[0]\n",
    "    z = x - centre\n",
    "    z = 2*z/span\n",
    "    Phi = np.zeros((x.shape[0], num_basis))\n",
    "    for i in range(num_basis):\n",
    "        Phi[:, i:i+1] = z**i\n",
    "    return Phi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x[:, 0] = np.linspace(1888, 2020, 1000)\n",
    "fig, ax = plt.subplots(figsize=(12,4))\n",
    "\n",
    "ax.plot(x, y, 'rx')\n",
    "display_prediction(basis=dict(radial=radial, polynomial=polynomial, fourier=fourier, relu=relu), \n",
    "                                 ylim=[-2.0, 4.],\n",
    "                                 data_limits=(1888, 2020),\n",
    "                                 fig=fig, ax=ax,\n",
    "                                 offset=0.,\n",
    "                                 wlim = (-4, 4),\n",
    "                                 num_basis=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Use the tool provided above to try and find the best fit you can to the data. Explore the parameter space and give the weight values you used for the \n",
    "\n",
    "(a) polynomial basis\n",
    "(b) RBF basis\n",
    "(c) Fourier basis\n",
    "\n",
    "Write your answers in the code box below creating a new vector of parameters (in the correct order!) for each basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3 Answer Code\n",
    "\n",
    "# (a) polynomial\n",
    "###### Edit these lines #####\n",
    "w_0 =\n",
    "w_1 = \n",
    "w_2 = \n",
    "w_3 =\n",
    "##############################\n",
    "w_polynomial = np.asarray([[w_0], [w_1], [w_2], [w_3]]) \n",
    "\n",
    "# (b) rbf\n",
    "###### Edit these lines #####\n",
    "w_0 =\n",
    "w_1 = \n",
    "w_2 = \n",
    "w_3 =\n",
    "##############################\n",
    "w_rbf = np.asarray([[w_0], [w_1], [w_2], [w_3]]) \n",
    "\n",
    "# (c) fourier\n",
    "###### Edit these lines #####\n",
    "w_0 =\n",
    "w_1 = \n",
    "w_2 = \n",
    "w_3 =\n",
    "##############################\n",
    "w_fourier = np.asarray([[w_0], [w_1], [w_2], [w_3]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray([[1, 2, 3, 4]]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We like to make use of *design* matrices for our data. Design matrices, as you will recall, involve placing the data points into rows of the matrix and data features into the columns of the matrix. By convention, we are referincing a vector with a bold lower case letter, and a matrix with a bold upper case letter. The design matrix is therefore given by\n",
    "$$\n",
    "\\boldsymbol{\\Phi} = \\begin{bmatrix} 1 & \\mathbf{x} & \\mathbf{x}^2\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "### Non-linear but linear in the Parameters\n",
    "\n",
    "One rather nice aspect of our model is that whilst it is non-linear in the inputs, it is still linear in the parameters $\\mathbf{w}$. This means that our derivations from before continue to operate to allow us to work with this model. In fact, although this is a non-linear regression it is still known as a *linear model* because it is linear in the parameters, \n",
    "$$\n",
    "f(\\mathbf{x}) = \\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x})\n",
    "$$\n",
    "where the vector $\\mathbf{x}$ appears inside the basis functions, making our result, $f(\\mathbf{x})$ non-linear in the inputs, but $\\mathbf{w}$ appears outside our basis function, making our result *linear* in the parameters. In practice, our basis function itself may contain its own set of parameters,\n",
    "$$\n",
    "f(\\mathbf{x}) = \\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}; \\boldsymbol{\\theta}),\n",
    "$$\n",
    "that we've denoted here as $\\boldsymbol{\\theta}$. If these parameters appear inside the basis function then our model is *non-linear* in these parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "For the following prediction functions state whether the model is linear in the inputs, the parameters or both.\n",
    "\n",
    "(a) $f(x) = w_1x_1 + w_2$\n",
    "\n",
    "(b) $f(x) = w_1\\exp(x_1) + w_2x_2 + w_3$\n",
    "\n",
    "(c) $f(x) = \\log(x_1^{w_1}) + w_2x_2^2 + w_3$\n",
    "\n",
    "(d) $f(x) = \\exp(-\\sum_i(x_i - w_i)^2)$\n",
    "\n",
    "(e) $f(x) = \\exp(-\\mathbf{w}^\\top \\mathbf{x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 Answer\n",
    "\n",
    "Write your answer to the question in this box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model Yourself\n",
    "\n",
    "You now have everything you need to fit a non-linear (in the inputs) basis function model to the marathon data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Choose one of the basis functions you have explored above. Compute the design matrix on the covariates (or input data), `x`. Use the design matrix and the response variable `y` to solve the following linear system for the model parameters `w`.\n",
    "$$\n",
    "\\boldsymbol{\\Phi}^\\top\\boldsymbol{\\Phi}\\mathbf{w} = \\boldsymbol{\\Phi}^\\top \\mathbf{y}\n",
    "$$\n",
    "Compute the corresponding error on the training data. How does it compare to the error you were able to achieve fitting the basis above? Plot the form of your prediction function from the least squares estimate alongside the form of you prediction function you fitted by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5 Answer Code\n",
    "# Write code for you answer to this question in this box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use of QR Decomposition for Numerical Stability\n",
    "\n",
    "In the last session we showed how rather than computing $\\mathbf{X}^\\top\\mathbf{X}$  as an intermediate step to our solution, we could compute the solution to the regressiond directly through [QR-decomposition](http://en.wikipedia.org/wiki/QR_decomposition). Now we will consider an example with non linear basis functions where such computation is critical for forming the right answer. \n",
    "\n",
    "Can you solve *Assignment Question 5* using QR decomposition?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "state": {
    "167ff508628448beab4a847772394dc3": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "3402afd50a9144bab45c98654f772c7e": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "35243b553cf34f3b854dfb6d5b28b666": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "c4d94cd2d5cf482fbe777df4784c9ca5": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "cd0dc121c68b4fa38fe3b64fdfc2a870": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
