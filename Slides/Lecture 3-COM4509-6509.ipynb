{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression \n",
    "\n",
    "## Machine Learning and Adaptive Intelligence\n",
    "\n",
    "### Mauricio Álvarez \n",
    "\n",
    "### Based on slides by Neil D. Lawrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pods\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review\n",
    "- Last time: Looked at objective functions for movie recommendation.\n",
    "- Minimized sum of squares objective by steepest descent and stochastic gradients.\n",
    "- This time: explore least squares for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression Examples\n",
    "\n",
    "-   Predict a real value, $y_i$ given some inputs\n",
    "    $x_i$.\n",
    "\n",
    "-   Predict quality of meat given spectral measurements (Tecator data).\n",
    "\n",
    "-   Radiocarbon dating, the C14 calibration curve: predict age given\n",
    "    quantity of C14 isotope.\n",
    "\n",
    "-   Predict quality of different Go or Backgammon moves given expert\n",
    "    rated training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Olympic 100m Data\n",
    "\n",
    "-  Gold medal times for Olympic 100 m runners since 1896.\n",
    "\n",
    "![image](./diagrams/100m_final_start.jpg)\n",
    "Image from Wikimedia Commons <http://bit.ly/191adDC>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Olympic 100m Data\n",
    "\n",
    "\n",
    "<img src=\"diagrams/male100.jpeg\" width=\"500\" height=\"40\" align=center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Olympic Marathon Data\n",
    "\n",
    "-   Gold medal times for Olympic Marathon since 1896.\n",
    "\n",
    "-   Marathons before 1924 didn’t have a standardised distance.\n",
    "\n",
    "-   Present results using pace per km.\n",
    "\n",
    "-   In 1904 Marathon was badly organised leading to very slow times.\n",
    "\n",
    "<img src=\"diagrams/Eliud_Kipchoge.jpg\" width=\"300\" height=\"40\" align=center>\n",
    "\n",
    "Image from Wikimedia Commons [Eliud Kipchoge](https://commons.wikimedia.org/wiki/File:Eliud_Kipchoge_in_Berlin_-_2015_(cropped).jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Olympic Marathon Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10e91ad68>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e91add8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pods.datasets.olympic_marathon_men()\n",
    "# Adding 2016 time\n",
    "np.append(data['X'], 2016)\n",
    "np.append(data['Y'], (2*60+8+(44/60))/42.195) # 2:08:44\n",
    "f, ax = plt.subplots(figsize=(7,7))\n",
    "ax.plot(data['X'], data['Y'], 'ro',markersize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is Machine Learning?\n",
    "\n",
    "$$ \\text{data} + \\text{model} = \\text{prediction}$$\n",
    "\n",
    "-   $\\text{data}$ : observations, could be actively or passively\n",
    "    acquired (meta-data).\n",
    "\n",
    "-   $\\text{model}$ : assumptions, based on previous experience (other data!\n",
    "    transfer learning etc), or beliefs about the regularities of\n",
    "    the universe. Inductive bias.\n",
    "\n",
    "-   $\\text{prediction}$ : an action to be taken or a categorization or a\n",
    "    quality score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression: Linear Releationship\n",
    "\n",
    "$$y_i = m x_i + c$$\n",
    "\n",
    "-   $y_i$ : winning time/pace.\n",
    "\n",
    "-   $x_i$ : year of Olympics.\n",
    "\n",
    "-   $m$ : rate of improvement over time.\n",
    "\n",
    "-   $c$ : winning time at year 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overdetermined System\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"diagrams/two_points.jpeg\" width=\"500\" height=\"40\" align=center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $y = mx + c$\n",
    "\n",
    "point 1: $x = 1$, $y=3$ $$3 = m + c$$ \n",
    "point 2: $x = 3$, $y=1$ $$1 = 3m + c$$ \n",
    "\n",
    "*We know how to solve this system of two equations and two unknowns*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"diagrams/two_points_plus_line.jpeg\" width=\"500\" height=\"40\" align=center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## We now observe a new point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"diagrams/three_points.jpeg\" width=\"500\" height=\"40\" align=center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $y = mx + c$\n",
    "\n",
    "point 1: $x = 1$, $y=3$ $$3 = m + c$$ \n",
    "point 2: $x = 3$, $y=1$ $$1 = 3m + c$$ \n",
    "point 3: $x = 2$, $y=2.5$ $$2.5 = 2m + c$$\n",
    "\n",
    "*Overdetermined system (more equations than unknowns): three equations and two unknowns* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"diagrams/Pierre-Simon_Laplace.png\" align=center width=40%>\n",
    "\n",
    "[Pierre-Simon Laplace](https://en.wikipedia.org/wiki/Pierre-Simon_Laplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $y = mx + c + \\epsilon$\n",
    "\n",
    "point 1: $x = 1$, $y=3$ \n",
    "$$3 = m + c + \\epsilon_1$$ \n",
    "\n",
    "point 2: $x = 3$, $y=1$ \n",
    "$$1 = 3m + c + \\epsilon_2$$ \n",
    "\n",
    "point 3: $x = 2$, $y=2.5$ \n",
    "$$2.5 = 2m + c + \\epsilon_3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Gaussian Density\n",
    "\n",
    "Perhaps the most common probability density.\n",
    "\n",
    "\\begin{align*}\n",
    "p(y| \\mu, \\sigma^2) & = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(y - \\mu)^2}{2\\sigma^2}\\right)\\\\\n",
    "                    & \\buildrel\\triangle\\over = \\mathcal{N}(y|\\mu, \\sigma^2)\n",
    "\\end{align*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gaussian Density\n",
    "\n",
    "![](./diagrams/gaussian_of_height.svg)\n",
    "\n",
    "The Gaussian PDF with $\\mu=1.7$ and variance $\\sigma^2=\n",
    "  0.0225$. Mean shown as red line. It could represent the heights of a population of\n",
    "  students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gaussian Density\n",
    "$$\n",
    "\\mathcal{N}(y|\\mu, \\sigma^2) =  \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "$\\sigma^2$ is the variance of the density and $\\mu$ is the mean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The IID Assumption\n",
    "\n",
    "- IID stands for Independent and Identically Distributed\n",
    "\n",
    "\n",
    "- We say that a set of random variables are IID if they are statistical independent AND each of them follows the same distribution.\n",
    "\n",
    "\n",
    "- Remember that a set of random variables is statistical independent if their joint distribution is equal to the product of their marginal distributions, $P(X, Y, Z) = P(X)P(Y)P(Z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Two Important Gaussian Properties\n",
    "\n",
    "**Sum of Gaussian**\n",
    "\n",
    "-   Sum of (i.i.d) Gaussian variables is also Gaussian.\n",
    "    \n",
    "    $$y_i \\sim \\mathcal{N}(\\mu_i, \\sigma^2_i)$$ \n",
    "    \n",
    "    And the sum is distributed as\n",
    "    \n",
    "    $$\\sum_{i=1}^{n} y_i \\sim \\mathcal{N}\\left(\\sum_{i=1}^n \\mu_i,\\sum_{i=1}^n \\sigma_i^2\\right)$$\n",
    "    \n",
    "    (*Aside*: As sum increases, sum of non-Gaussian, finite variance variables is\n",
    "    also Gaussian [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Two Important Gaussian Properties\n",
    "\n",
    "**Scaling a Gaussian**\n",
    "\n",
    "-   Scaling a Gaussian leads to a Gaussian.\n",
    "    \n",
    "    $$y \\sim \\mathcal{N}(\\mu, \\sigma^2)$$\n",
    "    \n",
    "    And the scaled density is distributed as\n",
    "    \n",
    "    $$w y \\sim \\mathcal{N}(w\\mu,w^2 \\sigma^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A probabilistic interpretation of the observations\n",
    "\n",
    "-   Set the mean of Gaussian to be a function.\n",
    "    \n",
    "    $$p\\left(y_i|x_i\\right)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp \\left(-\\frac{\\left(y_i-f\\left(x_i\\right)\\right)^{2}}\n",
    "    {2\\sigma^2}\\right).$$\n",
    "\n",
    "\n",
    "-   This gives us a ‘noisy function’.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Height as a Function of Weight\n",
    "\n",
    "-   In the standard Gaussian, parametized by mean and variance, make the mean a linear function of an *input*.\n",
    "\n",
    "-   This leads to a regression model. \n",
    "\n",
    "    \\begin{align*}\n",
    "               y_i  =  & \\,f\\left(x_i\\right)+\\epsilon_i,\\\\\n",
    "       \\epsilon_i \\sim &  \\mathcal{N}(0, \\sigma^2).\n",
    "     \\end{align*}\n",
    "        \n",
    "-   Assume $y_i$ is height and $x_i$ is weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Point Likelihood\n",
    "\n",
    "-   Likelihood of an individual data point\n",
    "    $$p\\left(y_i|x_i,m,c, \\sigma^2\\right)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\n",
    "             \\exp \\left(-\\frac{\\left(y_i-mx_i-c\\right)^{2}}{2\\sigma^2}\\right).$$\n",
    "    \n",
    "\n",
    "-   Parameters are gradient, $m$, offset, $c$ of the function and noise\n",
    "    variance $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Set Likelihood\n",
    "\n",
    "-   If the noise, $\\epsilon_i$ is sampled independently for each\n",
    "    data point.\n",
    "\n",
    "-   Each data point is independent (given $m$ and $c$).\n",
    "\n",
    "-   Define \n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbf{y} =\n",
    "                \\begin{bmatrix}\n",
    "                    y_1\\\\\n",
    "                    y_2\\\\\n",
    "                    \\vdots\\\\\n",
    "                    y_n\n",
    "                \\end{bmatrix},\n",
    "     \\quad\n",
    "     \\mathbf{x} =\n",
    "                \\begin{bmatrix}\n",
    "                    x_1\\\\\n",
    "                    x_2\\\\\n",
    "                    \\vdots\\\\\n",
    "                    x_n\n",
    "                \\end{bmatrix},\n",
    "\\end{align*}\n",
    "\n",
    "-   For independent variables:\n",
    "    $$p(\\mathbf{y}) = \\prod_{i=1}^n p(y_i)$$\n",
    "    $$p(\\mathbf{y}|\\mathbf{x}, m, c,\\sigma^2) = \\prod_{i=1}^n p(y_i|x_i, m, c, \\sigma^2)$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### For Gaussian \n",
    "\n",
    "- i.i.d. assumption\n",
    "\\begin{align*}\n",
    "                   p(\\mathbf{y}) &= \\prod_{i=1}^n p(y_i)\\\\\n",
    "   p(\\mathbf{y}|\\mathbf{x}, m, c, \\sigma^2)&= \\prod_{i=1}^n p(y_i|x_i, m, c, \\sigma^2)\\\\\n",
    "   p(\\mathbf{y}|\\mathbf{x}, m, c, \\sigma^2)&= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp \\left(-\\frac{\\left(y_i-mx_i-  \n",
    "                                     c\\right)^{2}}{2\\sigma^2}\\right)\\\\\n",
    "   p(\\mathbf{y}|\\mathbf{x}, m, c, \\sigma^2)&= \\frac{1}{\\left(2\\pi \\sigma^2\\right)^{\\frac{n}{2}}}\\exp \\left(-  \n",
    "                                      \\frac{\\sum_{i=1}^n\\left(y_i-mx_i-c\\right)^{2}}{2\\sigma^2}\\right).\n",
    " \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Log Likelihood Function\n",
    "\n",
    "- We can use the likelihood function, $p(\\mathbf{y}|\\mathbf{x}, m, c)$, \n",
    "\n",
    "     $$p(\\mathbf{y}|\\mathbf{x}, m, c)= \\frac{1}{\\left(2\\pi \\sigma^2\\right)^{\\frac{n}{2}}}\\exp \\left(-  \n",
    "                                      \\frac{\\sum_{i=1}^n\\left(y_i-mx_i-c\\right)^{2}}{2\\sigma^2}\\right),$$\n",
    "\n",
    " to estimate the parameters $m$, $c$ and $\\sigma^2$.\n",
    "\n",
    "- In practice, we prefer to work with the log likelihood:\n",
    "    \n",
    "    $$L(m,c,\\sigma^{2})=-\\frac{n}{2}\\log 2\\pi -\\frac{n}{2}\\log \\sigma^2 -\\sum _{i=1}^{n}\\frac{\\left(y_i-mx_i-    \n",
    "        c\\right)^{2}}{2\\sigma^2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Consistency of Maximum Likelihood\n",
    "\n",
    "\n",
    "-   If data was really generated according to the probability we specified, the correct parameters will be recovered   in the limit as $n \\rightarrow \\infty$.\n",
    "\n",
    "\n",
    "-   This can be proven through sample based approximations (law of large numbers) of “KL divergences”.\n",
    "\n",
    "\n",
    "-   Mainstay of classical statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Probabilistic Interpretation of the Error Function\n",
    "\n",
    "-   Probabilistic Interpretation for Error Function is Negative Log Likelihood.\n",
    "\n",
    "\n",
    "-   *Minimizing* error function is equivalent to *maximizing* log likelihood.\n",
    "\n",
    "\n",
    "-   Maximizing *log likelihood* is equivalent to maximizing the *likelihood* because $\\log$ is monotonic.\n",
    "\n",
    "\n",
    "-   Probabilistic interpretation: Minimizing error function is equivalent to maximum likelihood with respect to   parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Error Function\n",
    "\n",
    "- Remember the expression for the log likelihood function:\n",
    "    \n",
    "    $$L(m,c,\\sigma^{2})=-\\frac{n}{2}\\log 2\\pi -\\frac{n}{2}\\log \\sigma^2 -\\frac{1}\n",
    "    {2\\sigma^2}\\sum_{i=1}^{n}\\left(y_i-mx_i-c\\right)^{2}.$$\n",
    "\n",
    "-  The negative log likelihood is the error function \n",
    "\n",
    "    $$E(m,c,\\sigma^{2})=\\frac{n}{2}\\log \\sigma^2 +\\frac{1}{2\\sigma^2}\\sum _{i=1}^{n}\\left(y_i-         mx_i-c\\right)^{2},$$\n",
    "    \n",
    "   where we have omitted the term $\\frac{n}{2}\\log 2\\pi$ that does not depend on the   parameters.\n",
    "\n",
    "-   Learning proceeds by minimizing this error function for the data\n",
    "    set provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Connection: Sum of Squares Error\n",
    "\n",
    "-   Ignoring terms which don’t depend on $m$ and $c$ gives\n",
    "    \n",
    "    $$E(m, c) \\propto \\sum_{i=1}^n (y_i - f(x_i))^2$$\n",
    "    \n",
    "    where $f(x_i) = mx_i + c$.\n",
    "\n",
    "\n",
    "-   This is known as the *sum of squares* error function.\n",
    "\n",
    "\n",
    "-   Commonly used and it is closely associated with the Gaussian likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reminder\n",
    "\n",
    "- Two functions involved:\n",
    "\n",
    "  - Prediction function: $f(x_i)$\n",
    "\n",
    "  - Error, or Objective function: $E(m, c)$\n",
    "\n",
    "\n",
    "- Error function depends on parameters through prediction function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mathematical Interpretation\n",
    "\n",
    "-   What is the mathematical interpretation?\n",
    "\n",
    "    -   There is a cost function.\n",
    "\n",
    "    -   It expresses mismatch between your prediction and reality.\n",
    "        $$E(m, c)=\\sum_{i=1}^n \\left(y_i - mx_i -c\\right)^2$$\n",
    "\n",
    "    -   This is known as the sum of squares error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning is Optimization\n",
    "\n",
    "-   Learning consists of minimizing the cost function.\n",
    "\n",
    "-   At the minima the gradient is zero.\n",
    "\n",
    "-   **Coordinate descent**: find gradient in each coordinate and set to zero.\n",
    "    \\begin{align}\n",
    "     \\frac{\\text{d}E(m)}{\\text{d}m} &= -2\\sum_{i=1}^n x_i\\left(y_i- m x_i - c \\right)\\\\\n",
    "                                   0&= -2\\sum_{i=1}^n x_i\\left(y_i- m x_i - c \\right)\n",
    "    \\end{align}                               \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning is Optimization\n",
    "\n",
    "- Fixed point equation for $m$\n",
    "    $$0 =\n",
    "          -2\\sum_{i=1}^n x_iy_i\n",
    "          +2\\sum_{i=1}^n\n",
    "            m x_i^2 +2\\sum_{i=1}^n cx_i$$\n",
    "    $$m  =    \\frac{\\sum_{i=1}^n \\left(y_i\n",
    "          -c\\right)x_i}{\\sum_{i=1}^nx_i^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning is Optimization\n",
    "\n",
    "-   Learning consists of minimizing the cost function.\n",
    "\n",
    "-   At the minima the gradient is zero.\n",
    "\n",
    "-  **Coordinate descent**: find gradient in each coordinate and set to zero.\n",
    "     $$\\frac{\\text{d}E(c)}{\\text{d}c} = -2\\sum_{i=1}^n \\left(y_i- m x_i - c \\right)$$\n",
    "    \n",
    "     $$0 = -2\\sum_{i=1}^n\\left(y_i-m x_i - c \\right)$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning is Optimization\n",
    "\n",
    "- Fixed point equation for $c$\n",
    "    \\begin{align}\n",
    "        0 &= -2\\sum_{i=1}^n y_i +2\\sum_{i=1}^n m x_i +2n c\\\\\n",
    "        c &= \\frac{\\sum_{i=1}^n \\left(y_i-mx_i\\right)}{n}\n",
    "    \\end{align}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fixed Point Updates\n",
    "\n",
    "Worked example. \n",
    "   \n",
    "   \\begin{align}\n",
    "       c^{*}=&\\frac{\\sum _{i=1}^{n}\\left(y_i-m^{*}x_i\\right)}{n},\\\\\n",
    "       m^{*}=&\\frac{\\sum _{i=1}^{n}x_i\\left(y_i-c^{*}\\right)}{\\sum _{i=1}^{n}x_i^{2}},\\\\\n",
    "      \\left.\\sigma^2\\right.^{*}=&\\frac{\\sum _{i=1}^{n}\\left(y_i-m^{*}x_i-c^{*}\\right)^{2}}{n}\n",
    "  \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Important Concepts Not Covered\n",
    "\n",
    "-   Other optimization methods:\n",
    "\n",
    "    -   Second order methods, conjugate gradient, quasi-Newton\n",
    "        and Newton.\n",
    "\n",
    "    -   Effective heuristics such as momentum.\n",
    "\n",
    "-   Local vs global solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reading\n",
    "\n",
    "- Section 1.1-1.2 of Rogers and Girolami (2016) for fitting linear models. \n",
    "- Section 1.2.5 of Bishop (2006) up to equation 1.65.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multi-dimensional Inputs\n",
    "\n",
    "-   Multivariate functions involve more than one input.\n",
    "\n",
    "-   Height might be a function of weight and gender.\n",
    "\n",
    "-   There could be other contributory factors.\n",
    "\n",
    "-   Place these factors in a feature vector $\\mathbf{x}_i$.\n",
    "\n",
    "-   Linear function is now defined as\n",
    "    $$f(\\mathbf{x}_i) = \\sum_{j=1}^D w_j x_{i, j} + c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vector Notation\n",
    "\n",
    "-   Write in vector notation,\n",
    "    $$f(\\mathbf{x}_i) = \\mathbf{w}^\\top \\mathbf{x}_i + c$$\n",
    "    \n",
    "    \n",
    "\n",
    "-   We can absorb $c$ into $\\mathbf{w}$ by assuming extra input $x_0$\n",
    "    which is always 1.\n",
    "    \n",
    "    $$f(\\mathbf{x}_i) = \\mathbf{w}^\\top \\mathbf{x}_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Log Likelihood for Multivariate Regression\n",
    "\n",
    "-   The likelihood of a single data point is\n",
    "    $$p\\left(y_i|x_i, \\mathbf{w},\\sigma^2\\right)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\n",
    "        \\left(-\\frac{\\left(y_i-\\mathbf{w}^{\\top}\\mathbf{x}_i\\right)^{2}}{2\\sigma^2}\\right).$$\n",
    "\n",
    "-   Leading to a log likelihood for the data set of\n",
    "    $$L(\\mathbf{w},\\sigma^2)= -\\frac{n}{2}\\log \\sigma^2\n",
    "          -\\frac{n}{2}\\log 2\\pi -\\frac{\\sum\n",
    "            _{i=1}^{n}\\left(y_i-\\mathbf{w}^{\\top}\\mathbf{x}_i\\right)^{2}}{2\\sigma^2}.$$\n",
    "\n",
    "-   And a corresponding error function of\n",
    "    $$E(\\mathbf{w},\\sigma^2)= \\frac{n}{2}\\log\n",
    "          \\sigma^2 + \\frac{\\sum\n",
    "            _{i=1}^{n}\\left(y_i-\\mathbf{w}^{\\top}\\mathbf{x}_i\\right)^{2}}{2\\sigma^2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Expand the Brackets\n",
    "\n",
    "\\begin{align*}\n",
    "  E(\\mathbf{w},\\sigma^2)  = & \\frac{n}{2}\\log\\sigma^2 + \\frac{\\sum_{i=1}^{n}\\left(y_i- \n",
    "                               \\mathbf{w}^{\\top}\\mathbf{x}_i\\right)^{2}}{2\\sigma^2}.\\\\\n",
    "                          = & \\frac{n}{2}\\log \\sigma^2 + \\frac{1}{2\\sigma^2}\\sum           \n",
    "                              _{i=1}^{n}y_i^{2}-\\frac{1}{\\sigma^2}\\sum \n",
    "                              _{i=1}^{n}y_i\\mathbf{w}^{\\top}\\mathbf{x}_i+\\frac{1}\n",
    "                              {2\\sigma^2}\\sum_{i=1}^{n}                            \n",
    "                              \\mathbf{w}^{\\top}\\mathbf{x}_i\\mathbf{x}_i^{\\top}\\mathbf{w}.\\\\\n",
    "                         = & \\frac{n}{2}\\log \\sigma^2 + \\frac{1}{2\\sigma^2}\\sum \n",
    "                              _{i=1}^{n}y_i^{2}-\\frac{1}{\\sigma^2}\n",
    "                             \\mathbf{w}^\\top\\sum_{i=1}^{n}\\mathbf{x}_iy_i\n",
    "                             +\\frac{1}{2\\sigma^2} \n",
    "                           \\mathbf{w}^{\\top}\\left[\\sum_{i=1}^{n}\\mathbf{x}_i\n",
    "                           \\mathbf{x}_i^{\\top}\\right]\\mathbf{w}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multivariate Derivatives\n",
    "\n",
    "-   We will need some matrix calculus.\n",
    "\n",
    "\n",
    "-   For now some simple multivariate differentiation:\n",
    "    $$\\frac{\\text{d}{\\mathbf{w}^{\\top}}{\\mathbf{a}}}{\\text{d}\\mathbf{w}}=\\mathbf{a}$$\n",
    "    and\n",
    "    $$\\frac{\\text{d}\\mathbf{w}^{\\top}\\mathbf{A}\\mathbf{w}}{\\text{d}\\mathbf{w}}=\\left(\\mathbf{A}+\\mathbf{A}^{\\top}\\right)\\mathbf{w}$$\n",
    "    or if $\\mathbf{A}$ is symmetric (*i.e.*\n",
    "    $\\mathbf{A}=\\mathbf{A}^{\\top}$)\n",
    "    $$\\frac{\\text{d}\\mathbf{w}^{\\top}\\mathbf{A}\\mathbf{w}}{\\text{d}\\mathbf{w}}=2\\mathbf{A}\\mathbf{w}.$$\n",
    "    \n",
    "    \n",
    "- A good reference for matrix calculus is \"The Matrix Cookbook\" by K B Petersen and M S Pedersen (2012)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Differentiate\n",
    "\n",
    "The objective function is given as\n",
    "\n",
    "\\begin{align*}\n",
    "  E(\\mathbf{w},\\sigma^2)  = \\frac{n}{2}\\log \\sigma^2 + \\frac{1}{2\\sigma^2}\\sum \n",
    "                              _{i=1}^{n}y_i^{2}-\\frac{1}{\\sigma^2}\n",
    "                             \\mathbf{w}^\\top\\sum_{i=1}^{n}\\mathbf{x}_iy_i\n",
    "                             +\\frac{1}{2\\sigma^2} \n",
    "                           \\mathbf{w}^{\\top}\\left[\\sum_{i=1}^{n}\\mathbf{x}_i\n",
    "                           \\mathbf{x}_i^{\\top}\\right]\\mathbf{w}.\n",
    "\\end{align*}\n",
    "\n",
    "Differentiating with respect to the vector $\\mathbf{w}$ we obtain\n",
    "\n",
    "$$\\frac{\\partial E\\left(\\mathbf{w},\\sigma^2 \\right)}{\\partial \\mathbf{w}}=-\\frac{1}{\\sigma^2} \\sum _{i=1}^{n}\\mathbf{x}_iy_i+\\frac{1}{\\sigma^2} \\left[\\sum _{i=1}^{n}\\mathbf{x}_i\\mathbf{x}_i^{\\top}\\right]\\mathbf{w}$$\n",
    "\n",
    "Leading to\n",
    "$$\\mathbf{w}^{*}=\\left[\\sum _{i=1}^{n}\\mathbf{x}_i\\mathbf{x}_i^{\\top}\\right]^{-1}\\sum _{i=1}^{n}\\mathbf{x}_iy_i.$$\n",
    "\n",
    "Using matrix notation, it can be shown that:\n",
    "$$\\sum _{i=1}^{n}\\mathbf{x}_i\\mathbf{x}_i^\\top = \\mathbf{X}^\\top \\mathbf{X}\\qquad \\sum _{i=1}^{n}\\mathbf{x}_iy_i = \\mathbf{X}^\\top \\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $\\sum _{i=1}^{n}\\mathbf{x}_i\\mathbf{x}_i^\\top=\\mathbf{X}^{\\top}\\mathbf{X}$\n",
    "\n",
    "Let us write \n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbf{X} =\n",
    "                \\begin{bmatrix}\n",
    "                    \\mathbf{x}^{\\top}_1\\\\\n",
    "                    \\mathbf{x}^{\\top}_2\\\\\n",
    "                    \\vdots\\\\\n",
    "                    \\mathbf{x}^{\\top}_n\n",
    "                \\end{bmatrix},\n",
    "     \\quad\n",
    "     \\mathbf{X}^{\\top} =\n",
    "                \\begin{bmatrix}\n",
    "                    \\mathbf{x}_1\\; \\mathbf{x}_2\\; \\cdots \\; \\mathbf{x}_n\\\\               \n",
    "                \\end{bmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "We can then say that\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbf{X}^{\\top}\\mathbf{X} =\n",
    "                \\begin{bmatrix}\n",
    "                    \\mathbf{x}_1\\; \\mathbf{x}_2\\; \\cdots \\; \\mathbf{x}_n\\\\               \n",
    "                \\end{bmatrix} \n",
    "                \\begin{bmatrix}\n",
    "                    \\mathbf{x}^{\\top}_1\\\\\n",
    "                    \\mathbf{x}^{\\top}_2\\\\\n",
    "                    \\vdots\\\\\n",
    "                    \\mathbf{x}^{\\top}_n\n",
    "                \\end{bmatrix} =  \n",
    "                \\mathbf{x}_1\\mathbf{x}^{\\top}_1+\\mathbf{x}_2\\mathbf{x}^{\\top}_2+\\cdots+\\mathbf{x}_n\n",
    "                \\mathbf{x}^{\\top}_n = \\sum _{i=1}^{n}\\mathbf{x}_i\\mathbf{x}_i^\\top.\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Update Equations\n",
    "\n",
    "-   Update for $\\mathbf{w}^{*}$.\n",
    "    $$\\mathbf{w}^{*} = \\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{-1} \\mathbf{X}^\\top \\mathbf{y}$$\n",
    "\n",
    "\n",
    "-   The solution for $\\mathbf{w}^{*}$ exists if we can compute $\\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{-1}$. The    inverse can be computed as long as $\\mathbf{X}^\\top \\mathbf{X}$ is non-singular (e.g. determinat different from zero, or has full-rank).\n",
    "\n",
    "\n",
    "-   The equation for $\\left.\\sigma^2\\right.^{*}$ may also be found\n",
    "    $$\\left.\\sigma^2\\right.^{{*}}=\\frac{\\sum _{i=1}^{n}\\left(y_i-\\left.\\mathbf{w}^{*}\\right.^{\\top}\\mathbf{x}_i\\right)^{2}}{n}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reading\n",
    "\n",
    "- Section 1.3 of Rogers and Girolami (2016) for Matrix & Vector Review.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
