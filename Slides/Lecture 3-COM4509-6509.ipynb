{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression \n",
    "\n",
    "## Machine Learning and Adaptive Intelligence\n",
    "\n",
    "### Mauricio Álvarez \n",
    "\n",
    "### Based on slides by Neil D. Lawrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pods\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review\n",
    "- Last time: Looked at objective functions for movie recommendation.\n",
    "- Minimized sum of squares objective by steepest descent and stochastic gradients.\n",
    "- This time: explore least squares for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression Examples\n",
    "\n",
    "-   Predict a real value, $y_i$ given some inputs\n",
    "    $x_i$.\n",
    "\n",
    "-   Predict quality of meat given spectral measurements (Tecator data).\n",
    "\n",
    "-   Radiocarbon dating, the C14 calibration curve: predict age given\n",
    "    quantity of C14 isotope.\n",
    "\n",
    "-   Predict quality of different Go or Backgammon moves given expert\n",
    "    rated training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Olympic 100m Data\n",
    "\n",
    "-  Gold medal times for Olympic 100 m runners since 1896.\n",
    "\n",
    "![image](./diagrams/100m_final_start.jpg)\n",
    "Image from Wikimedia Commons <http://bit.ly/191adDC>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Olympic 100m Data\n",
    "\n",
    "\n",
    "<img src=\"diagrams/male100.jpeg\" width=\"500\" height=\"40\" align=center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Olympic Marathon Data\n",
    "\n",
    "-   Gold medal times for Olympic Marathon since 1896.\n",
    "\n",
    "-   Marathons before 1924 didn’t have a standardised distance.\n",
    "\n",
    "-   Present results using pace per km.\n",
    "\n",
    "-   In 1904 Marathon was badly organised leading to very slow times.\n",
    "\n",
    "<img src=\"diagrams/Eliud_Kipchoge.jpg\" width=\"300\" height=\"40\" align=center>\n",
    "\n",
    "Image from Wikimedia Commons [Eliud Kipchoge](https://commons.wikimedia.org/wiki/File:Eliud_Kipchoge_in_Berlin_-_2015_(cropped).jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Olympic Marathon Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pods.datasets.olympic_marathon_men()\n",
    "# Adding 2016 time\n",
    "np.append(data['X'], 2016)\n",
    "np.append(data['Y'], (2*60+8+(44/60))/42.195) # 2:08:44\n",
    "f, ax = plt.subplots(figsize=(7,7))\n",
    "ax.plot(data['X'], data['Y'], 'ro',markersize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is Machine Learning?\n",
    "\n",
    "$$ \\text{data} + \\text{model} = \\text{prediction}$$\n",
    "\n",
    "-   $\\text{data}$ : observations, could be actively or passively\n",
    "    acquired (meta-data).\n",
    "\n",
    "-   $\\text{model}$ : assumptions, based on previous experience (other data!\n",
    "    transfer learning etc), or beliefs about the regularities of\n",
    "    the universe. Inductive bias.\n",
    "\n",
    "-   $\\text{prediction}$ : an action to be taken or a categorization or a\n",
    "    quality score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression: Linear Releationship\n",
    "\n",
    "$$y_i = m x_i + c$$\n",
    "\n",
    "-   $y_i$ : winning time/pace.\n",
    "\n",
    "-   $x_i$ : year of Olympics.\n",
    "\n",
    "-   $m$ : rate of improvement over time.\n",
    "\n",
    "-   $c$ : winning time at year 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overdetermined System\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"diagrams/two_points.jpeg\" width=\"500\" height=\"40\" align=center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $y = mx + c$\n",
    "\n",
    "point 1: $x = 1$, $y=3$ $$3 = m + c$$ \n",
    "point 2: $x = 3$, $y=1$ $$1 = 3m + c$$ \n",
    "\n",
    "*We know how to solve this system of two equations and two unknowns*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"diagrams/two_points_plus_line.jpeg\" width=\"500\" height=\"40\" align=center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## We now observe a new point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"diagrams/three_points.jpeg\" width=\"500\" height=\"40\" align=center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $y = mx + c$\n",
    "\n",
    "point 1: $x = 1$, $y=3$ $$3 = m + c$$ \n",
    "point 2: $x = 3$, $y=1$ $$1 = 3m + c$$ \n",
    "point 3: $x = 2$, $y=2.5$ $$2.5 = 2m + c$$\n",
    "\n",
    "*Overdetermined system (more equations than unknowns): three equations and two unknowns* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"diagrams/Pierre-Simon_Laplace.png\" align=center width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $y = mx + c + \\epsilon$\n",
    "\n",
    "point 1: $x = 1$, $y=3$ \n",
    "$$3 = m + c + \\epsilon_1$$ \n",
    "\n",
    "point 2: $x = 3$, $y=1$ \n",
    "$$1 = 3m + c + \\epsilon_2$$ \n",
    "\n",
    "point 3: $x = 2$, $y=2.5$ \n",
    "$$2.5 = 2m + c + \\epsilon_3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Gaussian Density\n",
    "\n",
    "Perhaps the most common probability density.\n",
    "\n",
    "\\begin{align*}\n",
    "p(y| \\mu, \\sigma^2) & = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(y - \\mu)^2}{2\\sigma^2}\\right)\\\\\n",
    "                    & \\buildrel\\triangle\\over = \\mathcal{N}(y|\\mu, \\sigma^2)\n",
    "\\end{align*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gaussian Density\n",
    "\n",
    "![](./diagrams/gaussian_of_height.svg)\n",
    "\n",
    "The Gaussian PDF with $\\mu=1.7$ and variance $\\sigma^2=\n",
    "  0.0225$. Mean shown as red line. It could represent the heights of a population of\n",
    "  students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gaussian Density\n",
    "$$\n",
    "\\mathcal{N}(y|\\mu, \\sigma^2) =  \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "$\\sigma^2$ is the variance of the density and $\\mu$ is the mean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Two Important Gaussian Properties\n",
    "\n",
    "**Sum of Gaussian**\n",
    "\n",
    "-   Sum of Gaussian variables is also Gaussian.\n",
    "    \n",
    "    $$y_i \\sim \\mathcal{N}(\\mu_i, \\sigma^2_i)$$ \n",
    "    \n",
    "    And the sum is distributed as\n",
    "    \n",
    "    $$\\sum_{i=1}^{n} y_i \\sim \\mathcal{N}\\left(\\sum_{i=1}^n \\mu_i,\\sum_{i=1}^n \\sigma_i^2\\right)$$\n",
    "    \n",
    "    (*Aside*: As sum increases, sum of non-Gaussian, finite variance variables is\n",
    "    also Gaussian [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Two Important Gaussian Properties\n",
    "\n",
    "**Scaling a Gaussian**\n",
    "\n",
    "-   Scaling a Gaussian leads to a Gaussian.\n",
    "    \n",
    "    $$y \\sim \\mathcal{N}(\\mu, \\sigma^2)$$\n",
    "    \n",
    "    And the scaled density is distributed as\n",
    "    \n",
    "    $$w y \\sim \\mathcal{N}(w\\mu,w^2 \\sigma^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Laplace's Idea\n",
    "\n",
    "### A Probabilistic Process\n",
    "\n",
    "-   Set the mean of Gaussian to be a function.\n",
    "    \n",
    "    $$p\\left(y_i|x_i\\right)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp \\left(-\\frac{\\left(y_i-f\\left(x_i\\right)\\right)^{2}}\n",
    "    {2\\sigma^2}\\right).$$\n",
    "\n",
    "\n",
    "-   This gives us a ‘noisy function’.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Height as a Function of Weight\n",
    "\n",
    "-   In the standard Gaussian, parametized by mean and variance.\n",
    "\n",
    "-   Make the mean a linear function of an *input*.\n",
    "\n",
    "-   This leads to a regression model. \n",
    "\n",
    "    \\begin{align*}\n",
    "               y_i=    &  f\\left(x_i\\right)+\\epsilon_i,\\\\\n",
    "       \\epsilon_i \\sim &  \\mathcal{N}(0, \\sigma^2).\n",
    "     \\end{align*}\n",
    "        \n",
    "-   Assume $y_i$ is height and $x_i$ is weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Point Likelihood\n",
    "\n",
    "-   Likelihood of an individual data point\n",
    "    $$p\\left(y_i|x_i,m,c\\right)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp \\left(-\\frac{\\left(y_i-mx_i-c\\right)^{2}}{2\\sigma^2}\\right).$$\n",
    "    \n",
    "\n",
    "-   Parameters are gradient, $m$, offset, $c$ of the function and noise\n",
    "    variance $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Set Likelihood\n",
    "\n",
    "-   If the noise, $\\epsilon_i$ is sampled independently for each\n",
    "    data point.\n",
    "\n",
    "-   Each data point is independent (given $m$ and $c$).\n",
    "\n",
    "-   For independent variables:\n",
    "    $$p(\\mathbf{y}) = \\prod_{i=1}^n p(y_i)$$\n",
    "    $$p(\\mathbf{y}|\\mathbf{x}, m, c) = \\prod_{i=1}^n p(y_i|x_i, m, c)$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### For Gaussian \n",
    "\n",
    "- i.i.d. assumption\n",
    "\\begin{align*}\n",
    "                   p(\\mathbf{y}) &= \\prod_{i=1}^n p(y_i)\\\\\n",
    "   p(\\mathbf{y}|\\mathbf{x}, m, c)&= \\prod_{i=1}^n p(y_i|x_i, m, c)\\\\\n",
    "   p(\\mathbf{y}|\\mathbf{x}, m, c)&= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp \\left(-\\frac{\\left(y_i-mx_i-  \n",
    "                                     c\\right)^{2}}{2\\sigma^2}\\right)\\\\\n",
    "   p(\\mathbf{y}|\\mathbf{x}, m, c)&= \\frac{1}{\\left(2\\pi \\sigma^2\\right)^{\\frac{n}{2}}}\\exp \\left(-  \n",
    "                                      \\frac{\\sum_{i=1}^n\\left(y_i-mx_i-c\\right)^{2}}{2\\sigma^2}\\right).\n",
    " \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Log Likelihood Function\n",
    "\n",
    "- We can use the likelihood function, $p(\\mathbf{y}|\\mathbf{x}, m, c)$, \n",
    "\n",
    "     $$p(\\mathbf{y}|\\mathbf{x}, m, c)= \\frac{1}{\\left(2\\pi \\sigma^2\\right)^{\\frac{n}{2}}}\\exp \\left(-  \n",
    "                                      \\frac{\\sum_{i=1}^n\\left(y_i-mx_i-c\\right)^{2}}{2\\sigma^2}\\right),$$\n",
    "\n",
    " to estimate the parameters $m$ and $c$.\n",
    "\n",
    "- In practice, we prefer to work with the log likelihood:\n",
    "    \n",
    "    $$L(m,c,\\sigma^{2})=-\\frac{n}{2}\\log 2\\pi -\\frac{n}{2}\\log \\sigma^2 -\\sum _{i=1}^{n}\\frac{\\left(y_i-mx_i-    \n",
    "        c\\right)^{2}}{2\\sigma^2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Consistency of Maximum Likelihood\n",
    "\n",
    "\n",
    "-   If data was really generated according to the probability we specified, the correct parameters will be recovered   in the limit as $n \\rightarrow \\infty$.\n",
    "\n",
    "\n",
    "-   This can be proven through sample based approximations (law of large numbers) of “KL divergences”.\n",
    "\n",
    "\n",
    "-   Mainstay of classical statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Probabilistic Interpretation of the Error Function\n",
    "\n",
    "-   Probabilistic Interpretation for Error Function is Negative Log Likelihood.\n",
    "\n",
    "\n",
    "-   *Minimizing* error function is equivalent to *maximizing* log likelihood.\n",
    "\n",
    "\n",
    "-   Maximizing *log likelihood* is equivalent to maximizing the *likelihood* because $\\log$ is monotonic.\n",
    "\n",
    "\n",
    "-   Probabilistic interpretation: Minimizing error function is equivalent to maximum likelihood with respect to   parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Error Function\n",
    "\n",
    "- Remember the expression for the log likelihood function:\n",
    "    \n",
    "    $$L(m,c,\\sigma^{2})=-\\frac{n}{2}\\log 2\\pi -\\frac{n}{2}\\log \\sigma^2 -\\frac{1}\n",
    "    {2\\sigma^2}\\sum_{i=1}^{n}\\left(y_i-mx_i-c\\right)^{2}.$$\n",
    "\n",
    "-  The negative log likelihood is the error function \n",
    "\n",
    "    $$E(m,c,\\sigma^{2})=\\frac{n}{2}\\log \\sigma^2 +\\frac{1}{2\\sigma^2}\\sum _{i=1}^{n}\\left(y_i-         mx_i-c\\right)^{2},$$\n",
    "    \n",
    "   where we have omitted the term $\\frac{n}{2}\\log 2\\pi$ that does not depend on the   parameters.\n",
    "\n",
    "-   Learning proceeds by minimizing this error function for the data\n",
    "    set provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Connection: Sum of Squares Error\n",
    "\n",
    "-   Ignoring terms which don’t depend on $m$ and $c$ gives\n",
    "    \n",
    "    $$E(m, c) \\propto \\sum_{i=1}^n (y_i - f(x_i))^2$$\n",
    "    \n",
    "    where $f(x_i) = mx_i + c$.\n",
    "\n",
    "\n",
    "-   This is known as the *sum of squares* error function.\n",
    "\n",
    "\n",
    "-   Commonly used and is closely associated with the Gaussian likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reminder\n",
    "\n",
    "- Two functions involved:\n",
    "\n",
    "  - Prediction function: $f(x_i)$\n",
    "\n",
    "  - Error, or Objective function: $E(m, c)$\n",
    "\n",
    "\n",
    "- Error function depends on parameters through prediction function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mathematical Interpretation\n",
    "\n",
    "-   What is the mathematical interpretation?\n",
    "\n",
    "    -   There is a cost function.\n",
    "\n",
    "    -   It expresses mismatch between your prediction and reality.\n",
    "        $$E(m, c)=\\sum_{i=1}^n \\left(y_i - mx_i -c\\right)^2$$\n",
    "\n",
    "    -   This is known as the sum of squares error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning is Optimization\n",
    "\n",
    "-   Learning consists of minimizing the cost function.\n",
    "\n",
    "-   At the minima the gradient is zero.\n",
    "\n",
    "-   Coordinate ascent, find gradient in each coordinate and set to zero.\n",
    "    \\begin{align}\n",
    "     \\frac{\\text{d}E(m)}{\\text{d}m} &= -2\\sum_{i=1}^n x_i\\left(y_i- m x_i - c \\right)\\\\\n",
    "                                   0&= -2\\sum_{i=1}^n x_i\\left(y_i- m x_i - c \\right)\n",
    "    \\end{align}                               \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning is Optimization\n",
    "\n",
    "- Fixed point equation for $m$\n",
    "    $$0 =\n",
    "          -2\\sum_{i=1}^n x_iy_i\n",
    "          +2\\sum_{i=1}^n\n",
    "            m x_i^2 +2\\sum_{i=1}^n cx_i$$\n",
    "    $$m  =    \\frac{\\sum_{i=1}^n \\left(y_i\n",
    "          -c\\right)x_i}{\\sum_{i=1}^nx_i^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning is Optimization\n",
    "\n",
    "-   Learning consists of minimizing the cost function.\n",
    "\n",
    "-   At the minima the gradient is zero.\n",
    "\n",
    "-   Coordinate ascent, find gradient in each coordinate and set to zero.\n",
    "     $$\\frac{\\text{d}E(c)}{\\text{d}c} = -2\\sum_{i=1}^n \\left(y_i- m x_i - c \\right)$$\n",
    "    \n",
    "     $$0 = -2\\sum_{i=1}^n\\left(y_i-m x_i - c \\right)$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning is Optimization\n",
    "\n",
    "- Fixed point equation for $c$\n",
    "    \\begin{align}\n",
    "        0 &= -2\\sum_{i=1}^n y_i +2\\sum_{i=1}^n m x_i +2n c\\\\\n",
    "        c &= \\frac{\\sum_{i=1}^n \\left(y_i-mx_i\\right)}{n}\n",
    "    \\end{align}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fixed Point Updates\n",
    "\n",
    "Worked example. \n",
    "   \n",
    "   \\begin{align}\n",
    "       c^{*}=&\\frac{\\sum _{i=1}^{n}\\left(y_i-m^{*}x_i\\right)}{n},\\\\\n",
    "       m^{*}=&\\frac{\\sum _{i=1}^{n}x_i\\left(y_i-c^{*}\\right)}{\\sum _{i=1}^{n}x_i^{2}},\\\\\n",
    "      \\left.\\sigma^2\\right.^{*}=&\\frac{\\sum _{i=1}^{n}\\left(y_i-m^{*}x_i-c^{*}\\right)^{2}}{n}\n",
    "  \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Important Concepts Not Covered\n",
    "\n",
    "-   Other optimization methods:\n",
    "\n",
    "    -   Second order methods, conjugate gradient, quasi-Newton\n",
    "        and Newton.\n",
    "\n",
    "    -   Effective heuristics such as momentum.\n",
    "\n",
    "-   Local vs global solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reading\n",
    "\n",
    "- Section 1.1-1.2 of Rogers and Girolami (2016) for fitting linear models. \n",
    "- Section 1.2.5 of Bishop (2006) up to equation 1.65.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multi-dimensional Inputs\n",
    "\n",
    "-   Multivariate functions involve more than one input.\n",
    "\n",
    "-   Height might be a function of weight and gender.\n",
    "\n",
    "-   There could be other contributory factors.\n",
    "\n",
    "-   Place these factors in a feature vector $\\mathbf{x}_i$.\n",
    "\n",
    "-   Linear function is now defined as\n",
    "    $$f(\\mathbf{x}_i) = \\sum_{j=1}^D w_j x_{i, j} + c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vector Notation\n",
    "\n",
    "-   Write in vector notation,\n",
    "    $$f(\\mathbf{x}_i) = \\mathbf{w}^\\top \\mathbf{x}_i + c$$\n",
    "    \n",
    "    \n",
    "\n",
    "-   Can absorb $c$ into $\\mathbf{w}$ by assuming extra input $x_0$\n",
    "    which is always 1.\n",
    "    \n",
    "    $$f(\\mathbf{x}_i) = \\mathbf{w}^\\top \\mathbf{x}_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Log Likelihood for Multivariate Regression\n",
    "\n",
    "-   The likelihood of a single data point is\n",
    "    $$p\\left(y_i|x_i\\right)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\n",
    "        \\left(-\\frac{\\left(y_i-\\mathbf{w}^{\\top}\\mathbf{x}_i\\right)^{2}}{2\\sigma^2}\\right).$$\n",
    "\n",
    "-   Leading to a log likelihood for the data set of\n",
    "    $$L(\\mathbf{w},\\sigma^2)= -\\frac{n}{2}\\log \\sigma^2\n",
    "          -\\frac{n}{2}\\log 2\\pi -\\frac{\\sum\n",
    "            _{i=1}^{n}\\left(y_i-\\mathbf{w}^{\\top}\\mathbf{x}_i\\right)^{2}}{2\\sigma^2}.$$\n",
    "\n",
    "-   And a corresponding error function of\n",
    "    $$E(\\mathbf{w},\\sigma^2)= \\frac{n}{2}\\log\n",
    "          \\sigma^2 + \\frac{\\sum\n",
    "            _{i=1}^{n}\\left(y_i-\\mathbf{w}^{\\top}\\mathbf{x}_i\\right)^{2}}{2\\sigma^2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Expand the Brackets\n",
    "\n",
    "\\begin{align*}\n",
    "  E(\\mathbf{w},\\sigma^2)  = & \\frac{n}{2}\\log\\sigma^2 + \\frac{\\sum_{i=1}^{n}\\left(y_i- \n",
    "                               \\mathbf{w}^{\\top}\\mathbf{x}_i\\right)^{2}}{2\\sigma^2}\\\\\n",
    "                          = & \\frac{n}{2}\\log \\sigma^2 + \\frac{1}{2\\sigma^2}\\sum           \n",
    "                              _{i=1}^{n}y_i^{2}-\\frac{1}{\\sigma^2}\\sum \n",
    "                              _{i=1}^{n}y_i\\mathbf{w}^{\\top}\\mathbf{x}_i+\\frac{1}\n",
    "                              {2\\sigma^2}\\sum_{i=1}^{n}                            \n",
    "                              \\mathbf{w}^{\\top}\\mathbf{x}_i\\mathbf{x}_i^{\\top}\\mathbf{w} \n",
    "                             +\\text{const}.\\\\\n",
    "                         = & \\frac{n}{2}\\log \\sigma^2 + \\frac{1}{2\\sigma^2}\\sum \n",
    "                              _{i=1}^{n}y_i^{2}-\\frac{1}{\\sigma^2}\n",
    "                             \\mathbf{w}^\\top\\sum_{i=1}^{n}\\mathbf{x}_iy_i\n",
    "                             +\\frac{1}{2\\sigma^2} \n",
    "                           \\mathbf{w}^{\\top}\\left[\\sum_{i=1}^{n}\\mathbf{x}_i\n",
    "                           \\mathbf{x}_i^{\\top}\\right]\\mathbf{w} +\\text{const}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multivariate Derivatives\n",
    "\n",
    "-   We will need some multivariate calculus.\n",
    "\n",
    "\n",
    "-   For now some simple multivariate differentiation:\n",
    "    $$\\frac{\\text{d}{\\mathbf{w}^{\\top}}{\\mathbf{a}}}{\\text{d}\\mathbf{w}}=\\mathbf{a}$$\n",
    "    and\n",
    "    $$\\frac{\\mathbf{w}^{\\top}\\mathbf{A}\\mathbf{w}}{\\text{d}\\mathbf{w}}=\\left(\\mathbf{A}+\\mathbf{A}^{\\top}\\right)\\mathbf{w}$$\n",
    "    or if $\\mathbf{A}$ is symmetric (*i.e.*\n",
    "    $\\mathbf{A}=\\mathbf{A}^{\\top}$)\n",
    "    $$\\frac{\\text{d}\\mathbf{w}^{\\top}\\mathbf{A}\\mathbf{w}}{\\text{d}\\mathbf{w}}=2\\mathbf{A}\\mathbf{w}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Differentiate\n",
    "\n",
    "The objective function is given as\n",
    "\n",
    "\\begin{align*}\n",
    "  E(\\mathbf{w},\\sigma^2)  = \\frac{n}{2}\\log \\sigma^2 + \\frac{1}{2\\sigma^2}\\sum \n",
    "                              _{i=1}^{n}y_i^{2}-\\frac{1}{\\sigma^2}\n",
    "                             \\mathbf{w}^\\top\\sum_{i=1}^{n}\\mathbf{x}_iy_i\n",
    "                             +\\frac{1}{2\\sigma^2} \n",
    "                           \\mathbf{w}^{\\top}\\left[\\sum_{i=1}^{n}\\mathbf{x}_i\n",
    "                           \\mathbf{x}_i^{\\top}\\right]\\mathbf{w} +\\text{const}.\n",
    "\\end{align*}\n",
    "\n",
    "Differentiating with respect to the vector $\\mathbf{w}$ we obtain\n",
    "\n",
    "$$\\frac{\\partial E\\left(\\mathbf{w},\\sigma^2 \\right)}{\\partial \\mathbf{w}}=\\frac{1}{\\sigma^2} \\sum _{i=1}^{n}\\mathbf{x}_iy_i-\\frac{1}{\\sigma^2} \\left[\\sum _{i=1}^{n}\\mathbf{x}_i\\mathbf{x}_i^{\\top}\\right]\\mathbf{w}$$\n",
    "\n",
    "Leading to\n",
    "$$\\mathbf{w}^{*}=\\left[\\sum _{i=1}^{n}\\mathbf{x}_i\\mathbf{x}_i^{\\top}\\right]^{-1}\\sum _{i=1}^{n}\\mathbf{x}_iy_i,$$\n",
    "\n",
    "Using matrix notation, it can be shown that:\n",
    "$$\\sum _{i=1}^{n}\\mathbf{x}_i\\mathbf{x}_i^\\top = \\mathbf{X}^\\top \\mathbf{X}\\qquad \\sum _{i=1}^{n}\\mathbf{x}_iy_i = \\mathbf{X}^\\top \\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $\\sum _{i=1}^{n}\\mathbf{x}_i\\mathbf{x}_i^\\top=\\mathbf{X}^{\\top}\\mathbf{X}$\n",
    "\n",
    "Let us write \n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbf{X} =\n",
    "                \\begin{bmatrix}\n",
    "                    \\mathbf{x}^{\\top}_1\\\\\n",
    "                    \\mathbf{x}^{\\top}_2\\\\\n",
    "                    \\vdots\\\\\n",
    "                    \\mathbf{x}^{\\top}_n\n",
    "                \\end{bmatrix},\n",
    "     \\quad\n",
    "     \\mathbf{X}^{\\top} =\n",
    "                \\begin{bmatrix}\n",
    "                    \\mathbf{x}_1\\; \\mathbf{x}_2\\; \\cdots \\; \\mathbf{x}_n\\\\               \n",
    "                \\end{bmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "We can then say that\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbf{X}^{\\top}\\mathbf{X} =\n",
    "                \\begin{bmatrix}\n",
    "                    \\mathbf{x}_1\\; \\mathbf{x}_2\\; \\cdots \\; \\mathbf{x}_n\\\\               \n",
    "                \\end{bmatrix} \n",
    "                \\begin{bmatrix}\n",
    "                    \\mathbf{x}^{\\top}_1\\\\\n",
    "                    \\mathbf{x}^{\\top}_2\\\\\n",
    "                    \\vdots\\\\\n",
    "                    \\mathbf{x}^{\\top}_n\n",
    "                \\end{bmatrix} =  \n",
    "                \\mathbf{x}_1\\mathbf{x}^{\\top}_1+\\mathbf{x}_2\\mathbf{x}^{\\top}_2+\\cdots+\\mathbf{x}_n\n",
    "                \\mathbf{x}^{\\top}_n = \\sum _{i=1}^{n}\\mathbf{x}_i\\mathbf{x}_i^\\top.\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Update Equations\n",
    "\n",
    "-   Update for $\\mathbf{w}^{*}$.\n",
    "    $$\\mathbf{w}^{*} = \\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{-1} \\mathbf{X}^\\top \\mathbf{y}$$\n",
    "\n",
    "-   The equation for $\\left.\\sigma^2\\right.^{*}$ may also be found\n",
    "    $$\\left.\\sigma^2\\right.^{{*}}=\\frac{\\sum _{i=1}^{n}\\left(y_i-\\left.\\mathbf{w}^{*}\\right.^{\\top}\\mathbf{x}_i\\right)^{2}}{n}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reading\n",
    "\n",
    "- Section 1.3 of Rogers and Girolami (2016) for Matrix & Vector Review.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
