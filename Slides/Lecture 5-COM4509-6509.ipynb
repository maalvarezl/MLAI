{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generalisation\n",
    "\n",
    "## Machine Learning and Adaptive Intelligence\n",
    "\n",
    "### Mauricio √Ålvarez \n",
    "\n",
    "### Based on slides by Neil D. Lawrence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review\n",
    "- Last time: introduced basis functions.\n",
    "- Showed how to maximize the likelihood of a non-linear model that's linear in parameters.\n",
    "- Explored the different characteristics of different basis function models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Polynomial Fits to Olympics Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pods\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import mlai\n",
    "from matplotlib import pyplot as plt\n",
    "import teaching_plots as plot\n",
    "%matplotlib inline\n",
    "\n",
    "from ipywidgets import *\n",
    "def display_plots(filebase, directory=None, width=700, height=500, **kwargs):\n",
    "    \"\"\"Display a series of plots controlled by sliders. The function relies on Python string format functionality to index through a series of plots.\"\"\"\n",
    "    def show_figure(filebase, directory, **kwargs):\n",
    "        \"\"\"Helper function to load in the relevant plot for display.\"\"\"\n",
    "        filename = filebase.format(**kwargs)\n",
    "        if directory is not None:\n",
    "            filename = directory + '/' + filename           \n",
    "        display(HTML(\"<img src='{filename}'>\".format(filename=filename)))\n",
    "        \n",
    "    interact(show_figure, filebase=fixed(filebase), directory=fixed(directory), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "max_basis = 10\n",
    "basis = mlai.polynomial\n",
    "\n",
    "data = pods.datasets.olympic_marathon_men()\n",
    "x = data['X']\n",
    "y = data['Y']\n",
    "\n",
    "data_limits = [1892, 2020]\n",
    "num_data = x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.rmse_fit(x, y, param_name='num_basis', param_range=(1, max_basis+1), \n",
    "              model=mlai.LM, basis=basis, data_limits=data_limits, \n",
    "              xlim=data_limits, objective_ylim=[0, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "display_plots('olympic_LM_polynomial_num_basis{num_basis:0>3}.svg', \n",
    "                            directory='./diagrams', num_basis=(1, max_basis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overfitting\n",
    "- Increased number of basis functions we obtain a better 'fit' to the data.\n",
    "- How will the model perform on previously unseen data?\n",
    "- Let's consider predicting the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot.holdout_fit(x, y, param_name='num_basis', \n",
    "                 param_range=(1, max_basis+1), \n",
    "                 model=mlai.LM, basis=basis, data_limits=data_limits,\n",
    "                 permute=False, objective_ylim=[0, 20], xlim=data_limits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Extrapolation\n",
    "\n",
    "- Here we are training beyond where the model has learnt.\n",
    "- This is known as *extrapolation*.\n",
    "- Extrapolation is predicting into the future here, but could be:\n",
    "    - Predicting back to the unseen past (pre 1892)\n",
    "    - Spatial prediction (e.g. Cholera rates outside Manchester given rates inside Manchester)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Alan Turing\n",
    "- He was a formidable Marathon runner. \n",
    "- In 1946 he ran a time 2 hours 46 minutes.\n",
    "- What is the probability he would have won an Olympics if one had been held in 1946?  \n",
    "![Alan Turing, Times in the Times](http://www.turing.org.uk/turing/pi2/times2.gif)![Alan Turing running in 1946](http://www.turing.org.uk/turing/pi2/run.jpg)\n",
    "<center>*Alan Turing, in 1946 he was only 11 minutes slower than the winner of the 1948 games. Would he have won a hypothetical games held in 1946? Source: [Alan Turing Internet Scrapbook](http://www.turing.org.uk/scrapbook/run.html).*</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interpolation\n",
    "- Predicting the wining time for 1946 Olympics is *interpolation*.\n",
    "- This is because we have times from 1936 and 1948.\n",
    "- If we want a model for *interpolation* how can we test it?\n",
    "- One trick is to sample the validation set from throughout the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.holdout_fit(x, y, param_name='num_basis', param_range=(1, max_basis+1), \n",
    "                 model=mlai.LM, basis=basis, data_limits=data_limits, \n",
    "                 xlim=data_limits, prefix='olympic_val_inter', objective_ylim=[0.0, 2.], permute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "display_plots('olympic_val_inter_LM_polynomial_num_basis{num_basis:0>3}.svg', \n",
    "                            directory='./diagrams', num_basis=(1, max_basis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Choice of Validation Set\n",
    "\n",
    "- The choice of validation set should reflect how you will use the model in practice.\n",
    "- For extrapolation into the future we tried validating with data from the future.\n",
    "- For interpolation we chose validation set from data.\n",
    "- For different validation sets we could get different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Leave One Out Error\n",
    "- Take training set and remove one point.\n",
    "- Train on the remaining data.\n",
    "- Compute the error on the point you removed (which wasn't in the training data).\n",
    "- Do this for each point in the training set in turn.\n",
    "- Average the resulting error. \n",
    "- This is the leave one out error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#plot.loo_fit(x, y, param_name='num_basis', param_range=(1, max_basis+1),  \n",
    "#             model=mlai.LM, basis=basis, data_limits=data_limits, \n",
    "#             xlim=data_limits, objective_ylim=[0., 2.], prefix='olympic_loo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "display_plots('olympic_loo{part:0>3}_LM_polynomial_num_basis{num_basis:0>3}.svg', \n",
    "                            directory='./diagrams', num_basis=(1, max_basis), part=(0,x.shape[0]-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bias Variance Decomposition\n",
    "\n",
    "- Suppose that the data $y$ comes from $y = f^*(\\mathbf{x}) + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$.\n",
    "\n",
    "\n",
    "- $f^*(\\mathbf{x})$ is the true underlying function, but it is unknown. \n",
    "\n",
    "\n",
    "- We find a function $f(\\mathbf{x})$ that approximates $f^*(\\mathbf{x})$ as close as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bias Variance Decomposition\n",
    "\n",
    "The data is sampled from an unknow distribution $P(\\mathbf{x}, y)$. The expected *test error* follows as\n",
    "\n",
    "$$\\mathbb{E}\\left[ (y - f(\\mathbf{x}))^2 \\right]$$\n",
    "\n",
    "Decompose as\n",
    "\n",
    "$$\\mathbb{E}\\left[ (y - f(\\mathbf{x}))^2 \\right] = \\text{bias}\\left[f(\\mathbf{x})\\right]^2 + \\text{variance}\\left[f(\\mathbf{x})\\right] +\\sigma^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bias\n",
    "\n",
    "- Given by\n",
    "    $$\\text{bias}\\left[f(\\mathbf{x})\\right] = \\mathbb{E}\\left[f(\\mathbf{x})\\right] - f^*(\\mathbf{x})$$\n",
    "    \n",
    "- Error due to bias comes from a model that's too simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Variance\n",
    "\n",
    "- Given by\n",
    "    $$\\text{variance}\\left[f(\\mathbf{x})\\right] = \\mathbb{E}\\left[\\left(f(\\mathbf{x}) -  \\mathbb{E}\\left[f(\\mathbf{x})\\right]\\right)^2\\right]$$\n",
    "    \n",
    "    \n",
    "- Slight variations in the training set cause changes in the prediction. Error due to variance is error in the model due to an overly complex model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $k$ Fold Cross Validation\n",
    "\n",
    "- Leave one out error can be very time consuming.\n",
    "- Need to train your algorithm $n$ times.\n",
    "- An alternative: $k$ fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.cv_fit(x, y, param_name='num_basis', param_range=(1, max_basis+1),  \n",
    "               model=mlai.LM, basis=basis, data_limits=data_limits,\n",
    "               xlim=data_limits, objective_ylim=[0.,2.], num_parts=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "display_plots('olympic_{num_parts}'.format(num_parts=5) + 'cv{part:0>2}_LM_polynomial_num_basis{num_basis:0>3}.svg', \n",
    "                            directory='./diagrams', part=(0,4),num_basis=(1, max_basis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reading\n",
    "- Section 1.5 of Rogers and Girolami (2016)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
