{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Logistic Regression & PyTorch for Deep Learning\n",
    "\n",
    "## A: [*Logistic Regression* ](#partA); B: [*Linear Regression with PyTorch NN*](#partB)\n",
    "\n",
    "[**Haiping Lu**](http://staffwww.dcs.shef.ac.uk/people/H.Lu/) -  [COM4509/6509 MLAI2021](https://github.com/maalvarezl/MLAI) @ The University of Sheffield\n",
    "\n",
    "**Accompanying lectures**: [YouTube video lectures recorded in Year 2020/21.](https://www.youtube.com/watch?v=hnAoAYSzZYI&list=PLuRoUKdWifzwCTez18kp8qL6WLXZhxtm8)\n",
    "\n",
    "**Sources**: Part A is based on the [one neuron](https://github.com/cbernet/maldives/tree/master/one_neuron) notebooks by  [Colin Bernet](https://github.com/cbernet). Part B is based on the [PyTorch tutorial from CSE446, University of Washington](https://courses.cs.washington.edu/courses/cse446/18wi/sections/section7/446_pytorch_tutorial.html) and Lab 1 of my [SimplyDeep](https://github.com/haipinglu/SimplyDeep/)  notebooks.\n",
    "\n",
    "**Reproducibility**: This [seed module](https://github.com/pykale/pykale/blob/master/kale/utils/seed.py) shows how we set seed to ensure reproducibility in our [PyKale library](https://github.com/pykale/pykale). \n",
    "\n",
    "**Machine learning pipeline**: Take a look at the [PyKale](https://github.com/pykale/pykale) library for pipeline-based APIs. \n",
    "\n",
    "**Note**: Try to answer the *five* questions when you first see them rather than coming back after going through the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='partA'></a>A: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1. The sigmoid function \n",
    "\n",
    "The **sigmoid** or **logistic function** is essential in binary classification problems. It is expressed as\n",
    "$$\\sigma(z) = \\frac{1}{1+e^{-z}}$$\n",
    "and here is what it looks like in 1D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "# the bias: \n",
    "b = 0\n",
    "# the weight: \n",
    "w = 1\n",
    "\n",
    "def sigmoid(x1):\n",
    "    # z is a linear function of x1\n",
    "    z = w*x1 + b\n",
    "    return 1 / (1+np.exp(-z))\n",
    "\n",
    "# create an array of evenly spaced values\n",
    "linx = np.linspace(-10,10,51)\n",
    "plt.plot(linx, sigmoid(linx))\n",
    "b=5\n",
    "plt.plot(linx, sigmoid(linx))\n",
    "w=5\n",
    "plt.plot(linx, sigmoid(linx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 1\n",
    "\n",
    "What are the parameters (w and b) for each of the three curves orange (left), green (middle) and blue (right)?\n",
    "\n",
    "**Answer**: \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at this function in more details:\n",
    "\n",
    "* when $z$ goes to infinity, $e^{-z}$ goes to zero, and $\\sigma (z)$ goes to one.\n",
    "* when $z$ goes to minus infinity, $e^{-z}$ goes to infinity, and $\\sigma (z)$ goes to zero.\n",
    "* $\\sigma(0) = 0.5$, since $e^0=1$.\n",
    "\n",
    "It is important to note that the sigmoid is bound between 0 and 1, like a probability. And actually, in binary classification problems, the probability for an example to belong to a given category is produced by a sigmoid function. To classify our examples, we can simply use the output of the sigmoid: A given unknown example with value $x$ will be classified to category 1 if $\\sigma(z) > 0.5$, and to category 0 otherwise. \n",
    "\n",
    "**Excercise**: Now you can go back to the cell above, and play a bit with the `b` and `w` parameters, redoing the plot everytime you change one of these parameters. \n",
    "\n",
    "* $b$ is the **bias**. Changing the bias simply moves the sigmoid along the horizontal axis. For example, if you choose $b=1$ and $w=1$, then $z = wx + b = 0$ at $x=-1$, and that's where the sigmoid will be equal to 0.5\n",
    "* $w$ is the **weight** of variable $x$. If you increase it, the sigmoid evolves faster as a function of $x$ and gets sharper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2. Logistic regression as the simplest neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build the simplest neural network to classify our examples:\n",
    "\n",
    "* Each example has one variable, so we need 1 input node on the input layer\n",
    "* We're not going to use any hidden layer, as that would complicate the network \n",
    "* We have two categories, so the output of the network should be a single value between 0 and 1, which is the estimated probability $p$ for an example to belong to category 1. Then, the probability to belong to category 0 is simply $1-p$. Therefore, we should have a single output neuron, the only neuron in the network.\n",
    "\n",
    "The sigmoid function can be used in the output neuron. Indeed, it spits out a value between 0 and 1, and can be used as a classification probability as we have seen in the previous section.\n",
    "\n",
    "We can represent our network in the following way:\n",
    "\n",
    "![Neural network with 1 neuron](https://github.com/cbernet/maldives/raw/master/images/one_neuron.png)\n",
    "\n",
    "In the output neuron: \n",
    "\n",
    "* the first box performs a change of variable and computes the **weighted input** $z$ of the neuron\n",
    "* the second box applies the **activation function** to the weighted input. Here, we choose the sigmoid $\\sigma (z) = 1/(1+e^{-z})$ as an activation function\n",
    "\n",
    "This simple network has only 2 tunable parameters, the weight $w$ and the bias $b$, both used in the first box. We see in particular that when the bias is very large, the neuron will **always be activated**, whatever the input. On the contrary, for very negative biases, the neuron is **dead**. \n",
    "\n",
    "We can write the output simply as a function of $x$, \n",
    "\n",
    "$$f(x) = \\sigma(z) = \\sigma(wx+b)$$\n",
    "This is exactly the **logistic regression** classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 2\n",
    "\n",
    "How can we rewrit the logistic regression classifier above using a **single vectorial parameter** (i.e. one vector containing all parameters)?\n",
    "\n",
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3. Classifying 2D dataset with logistic regression\n",
    "\n",
    "Let's create a sample of examples with two values x1 and x2, with two categories. \n",
    "For category 0, the underlying probability distribution is a 2D Gaussian centered on (0,0), with width = 1 along both directions. For category 1, the Gaussian is centered on (2,2). We assign label 0 to category 0, and label 1 to category 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creation\n",
    "\n",
    "Let's create a sample of examples with two values x1 and x2, with two categories. \n",
    "For category 0, the underlying probability distribution is a 2D Gaussian centered on (0,0), with width = 1 along both directions. For category 1, the Gaussian is centered on (2,2). We assign label 0 to category 0, and label 1 to category 1. Check out the [documentation for Gaussian data generation](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.multivariate_normal.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = np.random.multivariate_normal\n",
    "# Number of samples \n",
    "nSamples = 500\n",
    "# (unit) variance:\n",
    "s2 = 1\n",
    "# below, we provide the coordinates of the mean as \n",
    "# a first argument, and then the covariance matrix\n",
    "# we generate nexamples examples for each category\n",
    "sgx0 = normal([0.,0.], [[s2, 0.], [0.,s2]], nSamples)\n",
    "sgx1 = normal([2.,2.], [[s2, 0.], [0.,s2]], nSamples)\n",
    "# setting the labels for each category\n",
    "sgy0 = np.zeros((nSamples,))\n",
    "sgy1 = np.ones((nSamples,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a scatter plot for the examples in the two categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sgx0[:,0], sgx0[:,1], alpha=0.5)\n",
    "plt.scatter(sgx1[:,0], sgx1[:,1], alpha=0.5)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to train a logistic regression to classify (x1,x2) points in one of the two categories depending on the values of x1 and x2. We form the dataset by concatenating the arrays of points, and also the arrays of labels for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgx = np.concatenate((sgx0, sgx1))\n",
    "sgy = np.concatenate((sgy0, sgy1))\n",
    "\n",
    "print(sgx.shape, sgy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D sigmoid\n",
    "\n",
    "In 2D, the expression of the sigmoid remains the same, but $z$ is now a function of the two variables $x_1$ and $x_2$, \n",
    "\n",
    "$$z=w_1 x_1 + w_2 x_2 + b$$\n",
    "\n",
    "And here is the code for the **2D sigmoid** and the defined function is called **sigmoid_2d**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "# bias: \n",
    "b = 0\n",
    "# x1 weight: \n",
    "w1 = 1\n",
    "# x2 weight:\n",
    "w2 = 2\n",
    "\n",
    "def sigmoid_2d(x1, x2):\n",
    "    # z is a linear function of x1 and x2\n",
    "    z = w1*x1 + w2*x2 + b\n",
    "    return 1 / (1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what this function looks like, we can make a 2D plot, with x1 on the horizontal axis, x2 on the vertical axis, and the value of the sigmoid represented as a color for each (x1, x2) coordinate. To do that, we will create an array of evenly spaced values along x1, and another array along x2. Taken together, these arrays will allow us to map the (x1,x2) plane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, xmax, npoints = (-6,6,51)\n",
    "linx1 = np.linspace(xmin,xmax,npoints)\n",
    "# no need for a new array, we just reuse the one we have with another name: \n",
    "linx2 = linx1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create a **meshgrid** from these arrays: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridx1, gridx2 = np.meshgrid(np.linspace(xmin,xmax,npoints), np.linspace(xmin,xmax,npoints))\n",
    "print(gridx1.shape, gridx2.shape)\n",
    "print('gridx1:')\n",
    "print(gridx1) \n",
    "print('gridx2')\n",
    "print(gridx2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you take the first line in both arrays, and scan the values on this line, you get: `(-6,-6), (-5.76, -6), (-5.52, -6)`... So we are scanning the x1 coordinates sequentially at the bottom of the plot. If you take the second line, you get: `(-6, -5.76), (-5.76, -5.76), (-5.52, -5.76)` ... : we are scanning the second line at the bottom of the plot, after moving up in x2 from one step. \n",
    "\n",
    "Scanning the full grid, you would scan the whole plot sequentially. \n",
    "\n",
    "Now we need to compute the value of the sigmoid for each pair (x1,x2) in the grid using the **sigmoid_2d** function defined above (cell 6). That's very easy to do with the output of `meshgrid`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = sigmoid_2d(gridx1, gridx2)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calls the `sigmoid_2d` function to each pair `(x1,y2)` taken from the `gridx1` and `gridx2` arrays so that we can plot our sigmoid in 2D: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(gridx1, gridx2, z)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2D sigmoid has the same kind of rising edge as the 1D sigmoid, but in 2D. \n",
    "With the parameters defined above: \n",
    "\n",
    "* The **weight** of $x_2$ is twice larger than the weight of $x_1$, so the sigmoid evolves twice faster as a function of $x_2$. \n",
    "* The separation boundary, which occurs for $z=0$, is a straight line with equation $w_1 x_1 + w_2 x_2 + b = 0$. Or equivalently: \n",
    "\n",
    "$$x_2 = -\\frac{w_1}{w_2} x_1 - \\frac{b}{w_2} = -0.5 x_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 3\n",
    "\n",
    "If you set one of the weights to zero, what will happen? Also, verify on the plot above that the equation above is indeed the one describing the separation boundary. \n",
    "\n",
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can test by editing the function `sigmoid_2d`, before re-executing the above cells. \n",
    "\n",
    "Note that if you prefer, you can plot the sigmoid in 3D like this:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_wireframe(gridx1,gridx2,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: change the parameters to observe how the 2D sigmoid changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression on the 2D data\n",
    "\n",
    "Let's now train a logistic regression to separate the two classes of examples. The goal of the training will be to use the existing examples to find the optimal values for the parameters $w_1, w_2, b$. \n",
    "\n",
    "We take the logistic regression algorithm from scikit-learn. \n",
    "Here, the logistic regression is used with the `lbfgs` solver. LBFGS is the minimization method used to find the best parameters. It is similar to [Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization). Since there is randomness, setting a seed is a good practice for **reproducibility**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "np.random.seed(2020) #set a seed for reproducibility\n",
    "clf = LogisticRegression(solver='lbfgs')  #clf: classifier\n",
    "clf.fit(sgx, sgy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note from the above that the default setting for logistic regression in scikit-learn uses L2 regularisation (penalty). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 4\n",
    "\n",
    "What is the objective of L2 regularisation (penalty)? *Hint*: this is not covered in lecture and you need to do some study (search).\n",
    "\n",
    "**Answer**: \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) to learn other options for penalty (regularisation) and other settings. In the simplest form, logistic regression does not have any hyperparameters but in practice, regularisation is often used, e.g. to reduce [overfitting](https://en.wikipedia.org/wiki/Overfitting).\n",
    "\n",
    "The logistic regression has been fitted (trained) to the data. Now, we can use it to predict the probability for any given (x1,x2) point to belong to category 1.\n",
    "\n",
    "We would like to plot this probability in 2D as a function of x1 and x2. To do that, we need to use the `clf.predict_proba` method which takes a 2D array of shape `(n_points, 2)`. The first dimension indexes the points, and the second one contains the values of x1 and x2. Again, we use our grid to map the (x1,x2) plane. But the gridx1 and gridx2 arrays defined above contain disconnected values of x1 and x2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gridx1.shape, gridx2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want is a 2D array of shape `(n_points, 2)`, not two 2D arrays of shape (51, 51)... \n",
    "So we need to **reshape** these arrays. First, we will **flatten** the gridx1 and gridx2 arrays so that all their values appear sequentially in a 1D array. Here is a small example to show how flatten works: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[0, 1], [2, 3]])\n",
    "print(a) \n",
    "print('flat array:', a.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will stitch the two 1D arrays together in two columns with np.c_ like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([[4, 5], [6, 7]])\n",
    "print(a.flatten())\n",
    "print(b.flatten())\n",
    "c = np.c_[a.flatten(), b.flatten()]\n",
    "print(c)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This array has exactly the shape expected by `clf.predict_proba`: a list of samples with two values. So let's do the same with our meshgrid, and let's compute the probabilities for all (x1,x2) pairs in the grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.c_[gridx1.flatten(), gridx2.flatten()]\n",
    "prob = clf.predict_proba(grid)\n",
    "prob.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, prob does not have the right shape to be plotted. Below, we will use a gridx1 and a gridx2 array with shapes (51,51). The shape of the prob array must also be (51,51), as the plotting method will simply map each (x1,x2) pair to a probability. So we need to **reshape** our probability array to shape (51,51). Reshaping works like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.array([0,1,2,3])\n",
    "print(d)\n",
    "print('reshaped to (2,2):')\n",
    "print(d.reshape(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally (!) we can do our plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that prob[:,1] returns, for all exemples, the probability p to belong to category 1. \n",
    "# prob[:,0] would return the probability to belong to category 0 (which is 1-p)\n",
    "plt.contourf(gridx1,gridx2,prob[:,1].reshape(npoints,npoints))\n",
    "plt.colorbar()\n",
    "plt.scatter(sgx0[:,0], sgx0[:,1], alpha=0.5)\n",
    "plt.scatter(sgx1[:,0], sgx1[:,1], alpha=0.5)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the logistic regression is able to separate these two classes well and the decision boundary is **linear**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='partB'></a>B: Linear Regression with PyTorch NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "* To perform linear regression using PyTorch for understanding the link between linear models and neural networks.\n",
    "\n",
    "**Suggested reading**: \n",
    "* What is PyTorch from [PyTorch tutorial](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py)\n",
    "\n",
    "#### Assumptions: basic python programming and [Anaconda](https://anaconda.org/) installed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why\n",
    "\n",
    "[Linear regression](https://en.wikipedia.org/wiki/Linear_regression) is a fundamental problem in statistics and machine learning. Using PyTorch, a deep learing library, to do linear regression will help bridge simple linear models with complex neural networks.\n",
    "\n",
    "## B1. PyTorch Installation and Basics\n",
    "\n",
    "### Install-1: direct installation (e.g., on your own machine with full installation right)\n",
    "\n",
    "#### Install [PyTorch](https://github.com/pytorch/pytorch) via [Anaconda](https://anaconda.org/)\n",
    "`conda install -c pytorch pytorch`\n",
    "\n",
    "When you are asked whether to proceed, say `y`\n",
    "\n",
    "#### Install [torchvision](https://github.com/pytorch/vision)\n",
    "`conda install -c pytorch torchvision`\n",
    "\n",
    "When you are asked whether to proceed, say `y`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install-2: Set up Anaconda Python environment (e.g., on a university desktop)\n",
    "\n",
    "On a university desktop, you may not have permission to install new packages on the main environment of Anaconda. Please follow the instructions below to set up a new environment. This is also recommended if you have different python projects running that may require different environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a command line terminal.\n",
    "\n",
    "**Create a new conda environment with Python 3.8**<br>\n",
    "`conda create -n mlai20 python=3.8 anaconda`\n",
    "\n",
    "**Activate the conda environment `mlai20`** (see [conda documentation](https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html#managing-environments))<br>\n",
    "\n",
    "For conda 4.6 and later versions: `conda activate mlai20`\n",
    "\n",
    "For conda versions prior to 4.6<br>\n",
    "`activate mlai20` (Windows)<br>\n",
    "`source activate mlai20` (Mac/Linux)<br><br>\n",
    "You will see `(mlai20)` on the left indciating your environment\n",
    "\n",
    "**Install Pytorch and Torchvision** (non-CUDA/GPU version for simplicity)<br>\n",
    "`conda install pytorch torchvision cpuonly -c pytorch`<br>\n",
    "If you have GPU, install the GPU version with command at [here](https://pytorch.org/)\n",
    "\n",
    "**Start Jupyter notebook server**: `jupyter notebook`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor\n",
    "Optional: Go over the first two modules of [PyTorch tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html), *What is PyTorch* and *Autograd*\n",
    "\n",
    "`torch.Tensor` is \n",
    "a multidimensional array data structure (array). You may check out the full list of [tensor types](http://pytorch.org/docs/master/tensors.html) and various [tensor operations](https://pytorch.org/docs/stable/torch.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Graph\n",
    "A computation graph defines/visualises a sequence of operations to go from input to model output. \n",
    "\n",
    "Consider a linear regression model $\\hat y = Wx + b$, where $x$ is our input, $W$ is a weight matrix, $b$ is a bias, and $\\hat y$ is the predicted output. As a computation graph, this looks like:\n",
    "\n",
    "![Linear Regression Computation Graph](https://imgur.com/IcBhTjS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch dynamically build the computational graph, for example\n",
    "![DynamicGraph.gif](https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/dynamic_graph.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B2. Linear Regression using PyTorch `nn` module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start right away with implementing linear regression in PyTorch to study PyTorch concepts closely. This part follows the [PyTorch Linear regression example](https://github.com/pytorch/examples/tree/master/regression) that trains a **single fully-connected layer** to fit a 4th degree polynomial.\n",
    "\n",
    "### A synthetic linear regression problem\n",
    "\n",
    "* Generate model parameters, weight and bias. The weight vector and bias are both tensors, 1D and 0D, respectively. We set a seed (2020) for **reproducibility**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(2020) # For reproducibility\n",
    "\n",
    "POLY_DEGREE = 4\n",
    "W_target = torch.randn(POLY_DEGREE, 1) * 5\n",
    "b_target = torch.randn(1) * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W_target)\n",
    "print(b_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, define a number of functions to generate the input (variables) and output (target/response). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(x):\n",
    "    \"\"\"Builds features i.e. a matrix with columns [x, x^2, x^3, x^4].\"\"\"\n",
    "    x = x.unsqueeze(1)\n",
    "    return torch.cat([x ** i for i in range(1, POLY_DEGREE+1)], 1)\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"Approximated function.\"\"\"\n",
    "    return x.mm(W_target) + b_target.item()\n",
    "\n",
    "def poly_desc(W, b):\n",
    "    \"\"\"Creates a string description of a polynomial.\"\"\"\n",
    "    result = 'y = '\n",
    "    for i, w in enumerate(W):\n",
    "        result += '{:+.2f} x^{} '.format(w, i + 1)\n",
    "    result += '{:+.2f}'.format(b[0])\n",
    "    return result\n",
    "\n",
    "def get_batch(batch_size=32):\n",
    "    \"\"\"Builds a batch i.e. (x, f(x)) pair.\"\"\"\n",
    "    random = torch.randn(batch_size)\n",
    "    x = make_features(random)\n",
    "    y = f(x)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a simple(st) neural network, which is a **single fully connected** (**FC**) layer. See [`torch.nn.Linear`](https://pytorch.org/docs/master/nn.html#torch.nn.Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = torch.nn.Linear(W_target.size(0), 1)\n",
    "print(fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This is a *network* with four input units, one output unit, with a bias term.\n",
    "    \n",
    "* Now generate the data. Let us try to get five pairs of (x,y) first to inspect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x, sample_y = get_batch(5)\n",
    "print(sample_x)\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Take a look at the FC layer weights (randomly initialised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fc.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Reset the gradients to zero, perform a forward pass to get prediction, and compute the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc.zero_grad()\n",
    "output = F.smooth_l1_loss(fc(sample_x), sample_y)\n",
    "loss = output.item()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Random did not give a good prediction. Let us do a backpropagation and update model parameters with gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.backward() \n",
    "for param in fc.parameters():  \n",
    "    param.data.add_(-0.1 * param.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Check the updated weights and respective loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fc.weight)\n",
    "output = F.smooth_l1_loss(fc(sample_x), sample_y)\n",
    "loss = output.item()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * Now keep feeding more data until the loss is small enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "for batch_idx in count(1):\n",
    "    # Get data\n",
    "    batch_x, batch_y = get_batch()\n",
    "\n",
    "    # Reset gradients\n",
    "    fc.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    output = F.smooth_l1_loss(fc(batch_x), batch_y)\n",
    "    loss = output.item()\n",
    "\n",
    "    # Backward pass\n",
    "    output.backward()\n",
    "\n",
    "    # Apply gradients\n",
    "    for param in fc.parameters():\n",
    "        param.data.add_(-0.1 * param.grad.data)\n",
    "\n",
    "    # Stop criterion\n",
    "    if loss < 1e-3:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loss: {:.6f} after {} batches'.format(loss, batch_idx))\n",
    "print('==> Learned function:\\t' + poly_desc(fc.weight.view(-1), fc.bias))\n",
    "print('==> Actual function:\\t' + poly_desc(W_target.view(-1), b_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 5\n",
    "\n",
    "Implement logistic regression using PyTorch (the `torch.nn.Module`) and apply it to the above synthetic 2D data (or a real dataset if you want it to be more challenging) for classification. You may also vary the synthetic data to observe performance variation. Check out [**reproducibility** in PyTorch](https://pytorch.org/docs/stable/notes/randomness.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 6\n",
    "\n",
    "Run a [cardiovascular disease diagnosis pipeline in Google Colab](https://colab.research.google.com/github/pykale/pykale/blob/main/examples/cmri_mpca/tutorial.ipynb) using [cardiac MRI data from Sheffield hospitals](https://github.com/pykale/data/tree/main/images/ShefPAH-179) to understand the steps involved in this machine learning pipeline developed here at Sheffield.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional ideas to explore\n",
    "\n",
    "* Change the [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions) to different choices and compare the results.  \n",
    "* Formulate another regression problem and solve it using `torch.nn`\n",
    "* Compare the `torch.nn` solution against the closed-form solution\n",
    "* Explore any other variations that you can think of to learn more\n",
    "* Explore more advanced examples at the [PyKale library](https://github.com/pykale/pykale/tree/master/examples) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "687c7bd9bfc30de0eb499b86a667cff972914d7d186d5367031e44cea3330a10"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('env1': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
