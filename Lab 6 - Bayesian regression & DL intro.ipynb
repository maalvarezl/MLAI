{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Bayesian Regression & PyTorch\n",
    "## A: *Bayesian Treatment of Linear Regression* \n",
    "## B: *Linear Regression with PyTorch*\n",
    "\n",
    "### [Haiping Lu](http://staffwww.dcs.shef.ac.uk/people/H.Lu/) -  COM4509/6509 MLAI2019\n",
    "\n",
    "### Part 6A: modified in Nov 2019 based on slides by Neil Lawrence [MLAI2015](http://inverseprobability.com/mlai2015/)\n",
    "### Part 6B: based on the [SimplyDeep](https://github.com/haipinglu/SimplyDeep/) project I recently started\n",
    "\n",
    "$$\\newcommand{\\inputScalar}{x}\n",
    "\\newcommand{\\lengthScale}{\\ell}\n",
    "\\newcommand{\\mappingVector}{\\mathbf{w}}\n",
    "\\newcommand{\\gaussianDist}[3]{\\mathcal{N}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gaussianSamp}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\zerosVector}{\\mathbf{0}}\n",
    "\\newcommand{\\eye}{\\mathbf{I}}\n",
    "\\newcommand{\\dataStd}{\\sigma}\n",
    "\\newcommand{\\dataScalar}{y}\n",
    "\\newcommand{\\dataVector}{\\mathbf{y}}\n",
    "\\newcommand{\\dataMatrix}{\\mathbf{Y}}\n",
    "\\newcommand{\\noiseScalar}{\\epsilon}\n",
    "\\newcommand{\\noiseVector}{\\mathbf{\\epsilon}}\n",
    "\\newcommand{\\noiseMatrix}{\\mathbf{\\Epsilon}}\n",
    "\\newcommand{\\inputVector}{\\mathbf{x}}\n",
    "\\newcommand{\\kernelMatrix}{\\mathbf{K}}\n",
    "\\newcommand{\\basisMatrix}{\\mathbf{\\Phi}}\n",
    "\\newcommand{\\basisVector}{\\mathbf{\\phi}}\n",
    "\\newcommand{\\basisScalar}{\\phi}\n",
    "\\newcommand{\\expSamp}[1]{\\left<#1\\right>}\n",
    "\\newcommand{\\expDist}[2]{\\left<#1\\right>_{#2}}\n",
    "\\newcommand{\\covarianceMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\numData}{n}\n",
    "\\newcommand{\\mappingScalar}{w}\n",
    "\\newcommand{\\mappingFunctionScalar}{f}\n",
    "\\newcommand{\\mappingFunctionVector}{\\mathbf{f}}\n",
    "\\newcommand{\\meanVector}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\meanScalar}{\\mu}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Bayesian Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1. The Bayesian Approach\n",
    "\n",
    "The aim of this notebook is to study Bayesian approaches to regression. In the Bayesian approach we define a *prior* density over our parameters, $m$ and $c$ or more generally $\\mathbf{w}$. This prior distribution gives us a range of expected values for our parameter *before* we have seen the data. The objective in Bayesian inference is to then compute the *posterior* density which is the effect on the density of having observed the data. \n",
    "\n",
    "In standard probability notation we write the **prior distribution** as,\n",
    "$$p(\\mathbf{w}),$$\n",
    "so it is the *marginal* distribution for the parameters, i.e. the distribution we have for the parameters without any knowledge about the data. \n",
    "\n",
    "The **posterior distribution** is written as,\n",
    "$$p(\\mathbf{w}|\\mathbf{y}, \\mathbf{X}).$$\n",
    "So the posterior distribution is the *conditional* distribution for the parameters given the data (which in this case consists of pairs of observations including response variables (or targets), $y_i$, and covariates (or inputs) $\\mathbf{x}_i$, where we are allowing the inputs to be multivariate. \n",
    "\n",
    "The posterior is recovered from the prior using *Bayes' rule*. Which is simply a rewriting of the product rule. We can recover Bayes rule as follows. The product rule of probability tells us that the joint distribution is given as the product of the conditional and the marginal. Dropping the inputs from our conditioning for the moment we have,\n",
    "$$p(\\mathbf{w}, \\mathbf{y})=p(\\mathbf{y}|\\mathbf{w})p(\\mathbf{w}),$$\n",
    "where we see we have related the joint density to the prior density and the **likelihood** from our previous investigation of regression,\n",
    "$$p(\\mathbf{y}|\\mathbf{w}) = \\prod_{i=1}^n\\mathcal{N}(y_i | \\mathbf{w}^\\top \\mathbf{x}_i, \\sigma^2)$$\n",
    "which arises from the assumption that our observation is given by\n",
    "$$y_i = \\mathbf{w}^\\top \\mathbf{x}_i + \\epsilon_i.$$\n",
    "In other words this is the Gaussian likelihood we have been fitting by minimizing the sum of squares. Have a look at Session 3 notebook as a reminder.\n",
    "\n",
    "We've introduce the likelihood, but we don't have  relationship with the posterior, however, the product rule can also be written in the following way\n",
    "$$p(\\mathbf{w}, \\mathbf{y}) = p(\\mathbf{w}|\\mathbf{y})p(\\mathbf{y}),$$\n",
    "where here we have simply used the opposite conditioning. We've already introduced the *posterior* density above. This is the density that represents our belief about the parameters *after* observing the data. This is combined with the **marginal likelihood**, sometimes also known as the evidence. It is the marginal likelihood, because it is the original likelihood of the data with the parameters marginalised, $p(\\mathbf{y})$. Here it's conditioned on nothing, but in practice you should always remember that everything here is conditioned on things like model choice: which set of basis functions. Because it's a regression problem, its also conditioned on the inputs. Using the equality between the two different forms of the joint density  we recover\n",
    "$$p(\\mathbf{w}|\\mathbf{y}) = \\frac{p(\\mathbf{y}|\\mathbf{w})p(\\mathbf{w})}{p(\\mathbf{y})}$$\n",
    "where we divided both sides by $p(\\mathbf{y})$ to recover this result. Let's re-introduce the conditioning on the input locations (or covariates), $\\mathbf{X}$ to write the full form of Bayes' rule for the regression problem. \n",
    "$$p(\\mathbf{w}|\\mathbf{y}, \\mathbf{X}) = \\frac{p(\\mathbf{y}|\\mathbf{w}, \\mathbf{X})p(\\mathbf{w})}{p(\\mathbf{y}|\\mathbf{X})}$$\n",
    "where the posterior density for the parameters given the data is $p(\\mathbf{w}|\\mathbf{y}, \\mathbf{X})$, the marginal likelihood is $p(\\mathbf{y}|\\mathbf{X})$, the prior density is $p(\\mathbf{w})$ and our original regression likelihood is given by $p(\\mathbf{y}|\\mathbf{w}, \\mathbf{X})$. It turns out that to compute the posterior the only things we need to do are define the prior and the likelihood. The other term on the right hand side can be computed by *the sum rule*. It is one of the key equations of Bayesian inference, the expectation of the likelihood under the prior, this process is known as marginalisation,\n",
    "$$\n",
    "p(\\mathbf{y}|\\mathbf{X}) = \\int p(\\mathbf{y}|\\mathbf{w},\\mathbf{X})p(\\mathbf{w}) \\text{d}\\mathbf{w}\n",
    "$$\n",
    "The term marginalisation nicely interprets the description of the probability as the *marginal likelihood*, because it somewhat has the implication that the variable name has been removed, and (perhaps) written in the margin. Marginalisation of a variable goes from a likelihood where the variable is in place, to a new likelihood where **all possible values** of that variable (under the prior) have been **considered and weighted** in the integral. \n",
    "\n",
    "This implies that **all we need for specifying our model is to define the likelihood and the prior**. We already have our likelihood from our earlier discussion, so our focus now turns to the prior density.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2. The Prior Density and How to Sample from It\n",
    "\n",
    "Let's assume that the prior density is given by a zero mean Gaussian, which is independent across each of the parameters, \n",
    "$$\\mappingVector \\sim \\gaussianSamp{\\zerosVector}{\\alpha \\eye}$$\n",
    "In other words, we are assuming, for the prior, that each element of the parameters vector, $\\mappingScalar_i$, was drawn from a Gaussian density as follows\n",
    "$$\\mappingScalar_i \\sim \\gaussianSamp{0}{\\alpha}$$\n",
    "\n",
    "Let's start by assigning the parameter of the prior distribution, which is the variance of the prior distribution, $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set prior variance on w\n",
    "alpha = 4.\n",
    "# set the order of the polynomial basis set\n",
    "order = 5\n",
    "# set the noise variance\n",
    "sigma2 = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating samples from the model\n",
    "\n",
    "A very important aspect of probabilistic modelling is to *sample* from your model to see what type of assumptions you are making about your data. In this case that involves a two stage process.\n",
    "\n",
    "1. **Sample a candiate parameter vector from the prior.**\n",
    "2. **Place the candidate parameter vector in the likelihood and sample functions conditiond on that candidate vector.**\n",
    "3. **Repeat to try and characterise the type of functions you are generating.**\n",
    "\n",
    "Given a prior variance (as defined above) we can now  sample from the prior distribution and combine with a basis set to see what assumptions we are making about the functions *a priori* (i.e. before we've seen the data). \n",
    "\n",
    "### Prepare the data and specify the model\n",
    "Firstly we compute the basis function matrix. We will do it both for our training data, and for a range of prediction locations (`x_pred`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pods\n",
    "data = pods.datasets.olympic_marathon_men()\n",
    "x = data['X']\n",
    "y = data['Y']\n",
    "num_data = x.shape[0]\n",
    "num_pred_data = 100 # how many points to use for plotting predictions\n",
    "x_pred = np.linspace(1890, 2016, num_pred_data)[:, None] # input locations for predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now let's build the basis matrices. We define the polynomial basis as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial(x, degree, loc, scale):\n",
    "    degrees = np.arange(degree+1)\n",
    "    return ((x-loc)/scale)**degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = 1950.\n",
    "scale = 1.\n",
    "degree = 5. \n",
    "Phi_pred = polynomial(x_pred, degree=degree, loc=loc, scale=scale)\n",
    "Phi = polynomial(x, degree=degree, loc=loc, scale=scale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Gaussian-distributed Variables\n",
    "\n",
    "Now we will sample from the prior to produce a vector $\\mappingVector$ and use it to plot a function which is representative of our belief *before* we fit the data. To do this we are going to use the properties of the Gaussian density and obtain a sample from a **standard normal** (mean=0, variance=1) using the function `np.random.normal`.\n",
    "\n",
    "First, let's consider the case where we have one data point and one feature in our basis set. In otherwords $\\mappingFunctionVector$ would be a scalar, $\\mappingVector$ would be a scalar and $\\basisMatrix$ would be a scalar. In this case we have \n",
    "\n",
    "$$\\mappingFunctionScalar = \\basisScalar \\mappingScalar$$\n",
    "\n",
    "If $\\mappingScalar$ is drawn from a normal density, \n",
    "\n",
    "$$\\mappingScalar \\sim \\gaussianSamp{\\meanScalar_\\mappingScalar}{c_\\mappingScalar}$$\n",
    "\n",
    "and $\\basisScalar$ is a scalar value which we are given, then properties of the Gaussian density tell us that \n",
    "\n",
    "$$\\basisScalar \\mappingScalar \\sim \\gaussianSamp{\\basisScalar\\meanScalar_\\mappingScalar}{\\basisScalar^2c_\\mappingScalar}$$\n",
    "\n",
    "Let's test this out numerically. First we will draw 200 samples from a standard normal,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_vec = np.random.normal(size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the mean of these samples and their variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('w sample mean is ', w_vec.mean())\n",
    "print('w sample variance is ', w_vec.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are close to zero (the mean) and one (the variance) as you'd expect. Now compute the mean and variance of the scaled version,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = 7\n",
    "f_vec = phi*w_vec\n",
    "print('True mean should be phi*0 = 0.')\n",
    "print('True variance should be phi*phi*1 = ', phi*phi)\n",
    "print('f sample mean is ', f_vec.mean())\n",
    "print('f sample variance is ', f_vec.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you increase the number of samples then you will see that the sample mean and the sample variance begin to converge towards the true mean and the true variance. Obviously adding an offset to a sample from `np.random.normal` will change the mean. So if you want to sample from a Gaussian with mean `mu` and standard deviation `sigma` one way of doing it is to sample from the standard normal and scale and shift the result, so to sample a set of $\\mappingScalar$ from a Gaussian with mean $\\meanScalar$ and variance $\\alpha$,\n",
    "\n",
    "$$w \\sim \\gaussianSamp{\\meanScalar}{\\alpha}$$\n",
    "\n",
    "We can simply scale and offset samples from the *standard normal*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 4 # mean of the distribution\n",
    "alpha = 2 # variance of the distribution\n",
    "w_vec = np.random.normal(size=200)*np.sqrt(alpha) + mu\n",
    "print('w sample mean is ', w_vec.mean())\n",
    "print('w sample variance is ', w_vec.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the `np.sqrt` is necesssary because we need to multiply by the standard deviation and we specified the variance as `alpha`. So scaling and offsetting a Gaussian distributed variable keeps the variable Gaussian, but it effects the mean and variance of the resulting variable. \n",
    "\n",
    "To get an idea of the overal shape of the resulting distribution, let's do the same thing with a histogram of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First the standard normal\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "z_vec = np.random.normal(size=1000) # by convention, in statistics, z is often used to denote samples from the standard normal\n",
    "w_vec = z_vec*np.sqrt(alpha) + mu\n",
    "# plot normalized histogram of w, and then normalized histogram of z on top\n",
    "plt.hist(w_vec, bins=30, density=True)\n",
    "plt.hist(z_vec, bins=30, density=True)\n",
    "plt.legend(('$w$', '$z$'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now **re-run** this histogram with 100,000 samples and check that the both histograms look qualitatively Gaussian.\n",
    "\n",
    "### Sampling from the Prior by scaling a standard Gaussian\n",
    "\n",
    "Let's use this way of constructing samples from a Gaussian to check what functions look like *a priori*. The process will be as follows. First, we sample a random vector of $K$ dimensional from `np.random.normal`. Then we scale it by $\\sqrt{\\alpha}$ to obtain a prior sample of $\\mappingVector$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = int(degree) + 1\n",
    "z_vec = np.random.normal(size=K)\n",
    "w_sample = z_vec*np.sqrt(alpha)\n",
    "print(w_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can combine our sample from the prior with the basis functions to create a function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_sample = np.dot(Phi_pred,w_sample)\n",
    "plt.plot(x_pred.flatten(), f_sample.flatten(), 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the recurring problem with the polynomial basis. Our prior allows relatively large coefficients for the basis associated with high polynomial degrees. Because we are operating with input values of around 2000, this leads to output functions of very high values. The fix we have used for this before is to rescale our data before we apply the polynomial basis to it. Above, we set the scale of the basis to 1. Here let's set it to 100 and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 100.\n",
    "Phi_pred = polynomial(x_pred, degree=degree, loc=loc, scale=scale)\n",
    "Phi = polynomial(x, degree=degree, loc=loc, scale=scale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to recompute the basis functions from above, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_sample = np.dot(Phi_pred,w_sample)\n",
    "plt.plot(x_pred.flatten(), f_sample.flatten(), 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's loop through some samples and plot various functions as samples from this system,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "K = int(degree)+1\n",
    "for i in range(num_samples):\n",
    "    z_vec = np.random.normal(size=K)\n",
    "    w_sample = z_vec*np.sqrt(alpha)\n",
    "    f_sample = np.dot(Phi_pred,w_sample)\n",
    "    plt.plot(x_pred.flatten(), f_sample.flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next: predicting the output \n",
    "\n",
    "The predictions for the mean output can now be computed. We want the **expected value of the predictions under the posterior distribution**. In matrix form, the predictions can be computed as\n",
    "\n",
    "$$\\mathbf{f} = \\basisMatrix \\mappingVector.$$\n",
    "\n",
    "This involves a matrix multiplication between a fixed matrix $\\basisMatrix$ and a vector $\\mappingVector$ that is drawn from a distribution. Because $\\mappingVector$ is drawn from a distribution, this imples that $\\mappingFunctionVector$ should also be drawn from a distribution. There are two distributions we are interested in though. We have just been sampling from the *prior* distribution to see what sort of functions we get *before* looking at the data. In Bayesian inference, we need to **compute the posterior distribution and sample from that density**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3. Bayesian Inference\n",
    "The process of Bayesian inference involves combining the prior, $p(\\mappingVector)$ with the likelihood, $p(\\dataVector|\\inputVector, \\mappingVector)$ to form the posterior, $p(\\mappingVector | \\dataVector, \\inputVector)$ through Bayes' rule,\n",
    "\n",
    "$$p(\\mappingVector|\\dataVector, \\inputVector) = \\frac{p(\\dataVector|\\inputVector, \\mappingVector)p(\\mappingVector)}{p(\\dataVector)}$$\n",
    "\n",
    "We've looked at the samples for our function $\\mappingFunctionVector = \\basisMatrix\\mappingVector$, which forms the mean of the Gaussian likelihood, under the prior distribution, i.e. we've sampled from $p(\\mappingVector)$ and multiplied the result by the basis matrix. With the Baye's rule above, we can sample from the posterior density, $p(\\mappingVector|\\dataVector, \\inputVector)$, and check that the new samples fit do correspond to the data, i.e. we can check that the updated distribution includes information from the data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Inference in the Univariate Case\n",
    "\n",
    "\n",
    "#### Prior Distribution\n",
    "\n",
    "Let's consider only one model parameter $c$. For linear regression, consider a Gaussian prior on the intercept:\n",
    "    $$c \\sim \\mathcal{N}(0, \\alpha_1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stages to Derivation of the Posterior\n",
    "\n",
    "-   Multiply likelihood by prior\n",
    "\n",
    "    -   They are \"exponentiated quadratics\", the answer is always also\n",
    "        an exponentiated quadratic because\n",
    "        $$\\exp(a^2)\\exp(b^2) = \\exp(a^2 + b^2)$$\n",
    "\n",
    "-   Complete the square to get the resulting density in the form of\n",
    "    a Gaussian.\n",
    "\n",
    "-   Recognise the mean and (co)variance of the Gaussian. This is the\n",
    "    estimate of the posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Trick\n",
    "\n",
    "$$p(c) = \\frac{1}{\\sqrt{2\\pi\\alpha_1}} \\exp\\left(-\\frac{1}{2\\alpha_1}c^2\\right)$$\n",
    "$$p(\\mathbf{y}|\\mathbf{x}, c, m, \\sigma^2) = \\frac{1}{\\left(2\\pi\\sigma^2\\right)^{\\frac{n}{2}}} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i - mx_i - c)^2\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(c| \\mathbf{y}, \\mathbf{x}, m, \\sigma^2) = \\frac{p(\\mathbf{y}|\\mathbf{x}, c, m, \\sigma^2)p(c)}{p(\\mathbf{y}|\\mathbf{x}, m, \\sigma^2)}$$\n",
    "\n",
    "$$p(c| \\mathbf{y}, \\mathbf{x}, m, \\sigma^2) =  \\frac{p(\\mathbf{y}|\\mathbf{x}, c, m, \\sigma^2)p(c)}{\\int p(\\mathbf{y}|\\mathbf{x}, c, m, \\sigma^2)p(c) \\text{d} c}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(c| \\mathbf{y}, \\mathbf{x}, m, \\sigma^2) \\propto  p(\\mathbf{y}|\\mathbf{x}, c, m, \\sigma^2)p(c)$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\log p(c | \\mathbf{y}, \\mathbf{x}, m, \\sigma^2) =&-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n(y_i-c - mx_i)^2-\\frac{1}{2\\alpha_1} c^2 + \\text{const}\\\\\n",
    "     = &-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i-mx_i)^2 -\\left(\\frac{n}{2\\sigma^2} + \\frac{1}{2\\alpha_1}\\right)c^2\\\\\n",
    "    & + c\\frac{\\sum_{i=1}^n(y_i-mx_i)}{\\sigma^2},\n",
    "  \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "complete the square of the quadratic form to obtain\n",
    "$$\\log p(c | \\mathbf{y}, \\mathbf{x}, m, \\sigma^2) = -\\frac{1}{2\\tau^2}(c - \\mu)^2 +\\text{const},$$\n",
    "where $\\tau^2 = \\left(n\\sigma^{-2} +\\alpha_1^{-1}\\right)^{-1}$\n",
    "and\n",
    "$\\mu = \\frac{\\tau^2}{\\sigma^2} \\sum_{i=1}^n(y_i-mx_i)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Bayesian Inference \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the posterior distribution for multivariate $\\mappingVector$, i.e., the posterior mean and *covariance*. This distribution is also Gaussian,\n",
    "\n",
    "$$p(\\mappingVector | \\dataVector, \\inputVector, \\dataStd^2) = \\mathcal{N}\\left(\\mappingVector|\\meanVector_\\mappingScalar, \\covarianceMatrix_\\mappingScalar\\right)$$\n",
    "\n",
    "with covariance, $\\covarianceMatrix_\\mappingScalar$, given by\n",
    "\n",
    "$$\\covarianceMatrix_\\mappingScalar = \\left(\\dataStd^{-2}\\basisMatrix^\\top \\basisMatrix + \\alpha^{-1} \\eye\\right)^{-1}$$ \n",
    "\n",
    "whilst the mean is given by\n",
    "\n",
    "$$\\meanVector_\\mappingScalar = \\covarianceMatrix_\\mappingScalar \\dataStd^{-2}\\basisMatrix^\\top \\dataVector$$\n",
    "\n",
    "Let's compute the posterior covariance and mean, then we'll sample from these densities to have a look at the posterior belief about $\\mappingVector$ once the data has been accounted for. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Compute the covariance for $\\mathbf{w}$ given the training data (using the formulae above) and call the resulting variable `w_cov`. Compute the mean for $\\mathbf{w}$ given the training data (using the formulae above) and call the resulting variable `w_mean`. Assume that $\\sigma^2 = 0.01$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1 Answer Code\n",
    "# Write code for you answer to this question in this box\n",
    "sigma2 = \n",
    "w_cov = \n",
    "w_mean = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from the Posterior\n",
    "\n",
    "Earlier, we were able to sample the prior values for the mean *independently* from a Gaussian using `np.random.normal` and scaling the result. However, observing the data *correlates* the parameters. Recall this from the first lab where we had a correlation between the offset, $c$ and the slope $m$ which caused such problems with the coordinate ascent algorithm. We need to sample from a *correlated* Gaussian. For this we can use `np.random.multivariate_normal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_sample = np.random.multivariate_normal(w_mean.flatten(), w_cov)\n",
    "f_sample = np.dot(Phi_pred,w_sample)\n",
    "plt.plot(x_pred.flatten(), f_sample.flatten(), 'r-')\n",
    "plt.plot(x, y, 'rx') # plot data to show fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's sample several functions and plot them all to see how the predictions fluctuate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_samples):\n",
    "    w_sample = np.random.multivariate_normal(w_mean.flatten(), w_cov)\n",
    "    f_sample = np.dot(Phi_pred,w_sample)\n",
    "    plt.plot(x_pred.flatten(), f_sample.flatten())\n",
    "plt.plot(x, y, 'rx') # plot data to show fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us an idea of what our predictions are. These are the predictions that are consistent with data and our prior. Try plotting different numbers of predictions. You can also try plotting beyond the range of where the data is and see what the functions do there. \n",
    "\n",
    "Rather than sampling from the posterior each time to compute our predictions, it might be better if we just summarised the predictions by the expected value of the output funciton, $f(x)$, for any particular input. If we can get formulae for this we don't need to sample the values of $f(x)$ we might be able to compute the distribution directly. Fortunately, in the Gaussian case, we can use properties of multivariate Gaussians to compute both the mean and the variance of these samples.\n",
    "\n",
    "### Properties of Gaussian Variables\n",
    "\n",
    "Gaussian variables have very particular properties, that many other densities don't exhibit. Perhaps foremost amoungst them is that the sum of any Gaussian distributed set of random variables also turns out to be Gaussian distributed. This property is much rarer than you might expect.\n",
    "\n",
    "#### Sum of Gaussian-distributed Variables\n",
    "\n",
    "The sum of Gaussian random variables is also Gaussian, so if we have a random variable $y_i$ drawn from a Gaussian density with mean $\\meanScalar_i$ and variance $\\dataStd^2_i$, \n",
    "\n",
    "$$y_i \\sim \\gaussianSamp{\\meanScalar_i}{\\dataStd^2_i}$$\n",
    "\n",
    "Then the sum of $K$ independently sampled values of $y_i$ will be drawn from a Gaussian with mean $\\sum_{i=1}^K \\mu_i$ and variance $\\sum_{i=1}^K \\dataStd_i^2$,\n",
    "\n",
    "\n",
    "$$\\sum_{i=1}^K y_i \\sim \\gaussianSamp{\\sum_{i=1}^K \\meanScalar_i}{\\sum_{i=1}^K \\dataStd_i^2}.$$\n",
    "\n",
    "Let's try that experimentally. First let's generate a vector of samples from a standard normal distribution, $z \\sim \\gaussianSamp{0}{1}$,  then we will scale and offset them, then keep adding them into a vector `y_vec`.\n",
    "\n",
    "#### Sampling from Gaussians and Summing Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10 # how many Gaussians to add.\n",
    "num_samples = 1000 # how many samples to have in y_vec\n",
    "mus = np.linspace(0, 5, K) # mean values generated linearly spaced between 0 and 5\n",
    "sigmas = np.linspace(0.5, 2, K) # sigmas generated linearly spaced between 0.5 and 2\n",
    "y_vec = np.zeros(num_samples)\n",
    "for mu, sigma in zip(mus, sigmas):\n",
    "    z_vec = np.random.normal(size=num_samples) # z is from standard normal\n",
    "    y_vec += z_vec*sigma + mu # add to y z*sigma + mu\n",
    "\n",
    "# now y_vec is the sum of each scaled and off set z.\n",
    "print('Sample mean is ', y_vec.mean(), ' and sample variance is ', y_vec.var())\n",
    "print('True mean should be ', mus.sum())\n",
    "print('True variance should be ', (sigmas**2).sum(), ' standard deviation ', np.sqrt((sigmas**2).sum())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can histogram `y_vec` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_vec, bins=30, normed=True)\n",
    "plt.legend('$y$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Multiplication of Gaussian Variables\n",
    "\n",
    "We are interested in what our model is saying about the sort of functions we are observing. The fact that summing of Gaussian variables leads to new Gaussian variables, and scaling of Gaussian variables *also* leads to Gaussian variables means that matrix multiplication (which is just a series of sums and scales) also leads to Gaussian densities. Matrix multiplication is just adding and scaling together, in the formula, $\\mappingFunctionVector = \\basisMatrix \\mappingVector$ we can extract the first element from $\\mappingFunctionVector$ as\n",
    "\n",
    "$$\\mappingFunctionScalar_i = \\basisVector_i^\\top \\mappingVector$$\n",
    "\n",
    "where $\\basisVector$ is a column vector from the $i$th row of $\\basisMatrix$ and $\\mappingFunctionScalar_i$ is the $i$th element of $\\mappingFunctionVector$. This vector inner product itself merely implies that \n",
    "\n",
    "$$\\mappingFunctionScalar_i = \\sum_{j=1}^K \\mappingScalar_j \\basisScalar_{i, j}$$\n",
    "\n",
    "and if we now say that $\\mappingScalar_i$ is Gaussian distributed, then because a scaled Gaussian is also Gaussian, and because a sum of Gaussians is also Gaussian, we know that $\\mappingFunctionScalar_i$ is also Gaussian distributed. It merely remains to work out its mean and covariance. The results are below and the derviations follow. For those interested, the derivations are  available [here](https://nbviewer.jupyter.org/github/lawrennd/mlai2015/blob/master/week6.ipynb).\n",
    "\n",
    "The expectation under the prior is given by\n",
    "\n",
    "$$\\expDist{\\mappingFunctionVector}{\\gaussianDist{\\mappingVector}{\\zerosVector}{\\alpha\\eye}} = \\zerosVector$$\n",
    "\n",
    "and the covariance is \n",
    "\n",
    "$$\\text{cov}\\left(\\mappingFunctionVector\\right)_{\\gaussianDist{\\mappingVector}{\\zerosVector}{\\alpha \\eye}} = \\alpha \\basisMatrix \\basisMatrix^\\top$$\n",
    "\n",
    "\n",
    "### Compute the Marginal Likelihood\n",
    "\n",
    "Since our observed output, $\\dataVector$, is given by a noise corrupted variation of $\\mappingFunctionVector$, the final distribution for $\\dataVector$ is given as \n",
    "\n",
    "$$\\dataVector = \\mappingFunctionVector + \\noiseVector$$\n",
    "\n",
    "where the noise, $\\noiseVector$, is sampled from a Gaussian density: $\\noiseVector \\sim \\gaussianSamp{\\zerosVector}{\\dataStd^2\\eye}$. So, in other words, we are taking a Gaussian distributed random value $\\mappingFunctionVector$,\n",
    "\n",
    "$$\\mappingFunctionVector \\sim \\gaussianSamp{\\zerosVector}{\\alpha\\basisMatrix\\basisMatrix^\\top}$$\n",
    "\n",
    "and adding to it another Gaussian distributed value, $\\noiseVector \\sim \\gaussianSamp{\\zerosVector}{\\dataStd^2\\eye}$, to form our data observations, $\\dataVector$. Once again the sum of two (multivariate) Gaussian distributed variables is also Gaussian, with a mean given by the sum of the means (both zero in this case) and the covariance given by the sum of the covariances. So we now have that the marginal likelihood for the data, $p(\\dataVector)$ is given by\n",
    "\n",
    "$$p(\\dataVector) = \\gaussianDist{\\dataVector}{\\zerosVector}{\\alpha \\basisMatrix \\basisMatrix^\\top + \\dataStd^2\\eye}$$\n",
    "\n",
    "This is our *implicit* assumption for $\\dataVector$ given our prior assumption for $\\mappingVector$.\n",
    "\n",
    "### Computing the Mean and Error Bars of the Functions\n",
    "\n",
    "These ideas together, now allow us to compute the mean and error bars of the predictions. The mean prediction, before corrupting by noise is given by,\n",
    "$$\n",
    "\\mathbf{f} = \\boldsymbol{\\Phi}\\mathbf{w}\n",
    "$$\n",
    "in matrix form. This gives you enough information to compute the predictive mean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Compute the predictive mean for the function at all the values of the basis function given by `Phi_pred`. Call the vector of predictions `f_pred_mean`. Plot the predictions alongside the data. We can also compute what the training error was. Use the output from your model to compute the predictive mean, and then compute the sum of squares error of that predictive mean.\n",
    "$$\n",
    "E = \\sum_{i=1}^n (y_i - \\langle f_i\\rangle)^2\n",
    "$$\n",
    "where $\\langle f_i\\rangle$ is the expected output of the model at point $x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2 Answer Code\n",
    "# Write code for you answer to this question in this box\n",
    "\n",
    "# compute mean under posterior density\n",
    "f_pred_mean = \n",
    "\n",
    "# plot the predictions\n",
    "\n",
    "# compute mean at the training data and sum of squares error\n",
    "f_mean = \n",
    "sum_squares = \n",
    "print('The error is: ', sum_squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Error Bars\n",
    "\n",
    "Finally, we can compute error bars for the predictions. The error bars are the standard deviations of the predictions for $\\mappingFunctionVector=\\basisMatrix\\mappingVector$ under the posterior density for $\\mappingVector$. The standard deviations of these predictions can be found from the variance of the prediction at each point. Those variances are the diagonal entries of the covariance matrix. We've already computed the form of the covariance under Gaussian expectations (see the derivations [here](https://nbviewer.jupyter.org/github/lawrennd/mlai2015/blob/master/week6.ipynb)), \n",
    "\n",
    "$$\\text{cov}\\left(\\mappingFunctionVector\\right)_{\\gaussianDist{\\mappingVector}{\\meanVector}{\\covarianceMatrix}} = \\basisMatrix\\covarianceMatrix \\basisMatrix^\\top$$\n",
    "\n",
    "which under the posterior density is given by\n",
    "\n",
    "$$\\text{cov}\\left(\\mappingFunctionVector\\right)_{\\gaussianDist{\\mappingVector}{\\meanVector_w}{\\covarianceMatrix_w}} = \\basisMatrix\\covarianceMatrix_w \\basisMatrix^\\top$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Question 3\n",
    "\n",
    "The error bars are given by computing the standard deviation of the predictions, $f$. For a given prediction $f_i$ the variance is $\\text{var}(f_i) = \\langle f_i^2\\rangle - \\langle f_i \\rangle^2$. This is given by the diagonal element of the covariance of $\\mathbf{f}$,\n",
    "$$\n",
    "\\text{var}(f_i) = \\boldsymbol{\\phi}_{i, :}^\\top \\mathbf{C}_w \\boldsymbol{\\phi}_{i, :}\n",
    "$$\n",
    "where $\\boldsymbol{\\phi}_{i, :}$ is the basis vector associated with the input location, $\\mathbf{x}_i$.\n",
    "\n",
    "Plot the mean function and the error bars for your basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3 Answer Code\n",
    "# Write code for you answer to this question in this box\n",
    "\n",
    "# Compute variance at function values\n",
    "f_pred_var = \n",
    "f_pred_std = \n",
    "\n",
    "# plot the mean and error bars at 2 standard deviations above and below the mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Linear Regression with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Objective\n",
    "\n",
    "* To perform linear regression using PyTorch for understanding the link between linear models and neural networks.\n",
    "\n",
    "**Suggested reading**: \n",
    "* What is PyTorch from [PyTorch tutorial](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py)\n",
    "\n",
    "#### Assumptions: basic python programming and [Anaconda](https://anaconda.org/) installed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why\n",
    "\n",
    "[Linear regression](https://en.wikipedia.org/wiki/Linear_regression) is a fundamental problem in statistics and machine learning. Using PyTorch, a deep learing library, to do linear regression will help bridge simple linear models with complex neural networks.\n",
    "\n",
    "## B1. PyTorch Installation and Basics\n",
    "\n",
    "#### Install [PyTorch](https://github.com/pytorch/pytorch) via [Anaconda](https://anaconda.org/)\n",
    "`conda install -c pytorch pytorch`\n",
    "\n",
    "When you are asked whether to proceed, say `y`\n",
    "\n",
    "#### Install [torchvision](https://github.com/pytorch/vision)\n",
    "`conda install -c pytorch torchvision`\n",
    "\n",
    "When you are asked whether to proceed, say `y`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: Go over the first two modules of [PyTorch tutorial](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py), *What is PyTorch* and *Autograd*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor\n",
    "`torch.Tensor` is \n",
    "a multidimensional array data structure (array). You may check out the full list of [tensor types](http://pytorch.org/docs/master/tensors.html) and various [tensor operations](https://pytorch.org/docs/stable/torch.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Graph\n",
    "A computation graph defines/visualises a sequence of operations to go from input to model output. \n",
    "\n",
    "Consider a linear regression model $\\hat y = Wx + b$, where $x$ is our input, $W$ is a weight matrix, $b$ is a bias, and $\\hat y$ is the predicted output. As a computation graph, this looks like:\n",
    "\n",
    "![Linear Regression Computation Graph](https://imgur.com/IcBhTjS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch dynamically build the computational graph, for example\n",
    "![DynamicGraph.gif](https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/dynamic_graph.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B2. Linear Regression using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start right away with implementing linear regression in PyTorch to study PyTorch concepts closely. This part follows the [PyTorch Linear regression example](https://github.com/pytorch/examples/tree/master/regression) that trains a **single fully-connected layer** to fit a 4th degree polynomial.\n",
    "\n",
    "### A synthetic linear regression problem\n",
    "\n",
    "* Generate model parameters, weight and bias. The weight vector and bias are both tensors, 1D and 0D, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "POLY_DEGREE = 4\n",
    "W_target = torch.randn(POLY_DEGREE, 1) * 5\n",
    "b_target = torch.randn(1) * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.7804],\n",
      "        [ 3.0660],\n",
      "        [ 1.6598],\n",
      "        [ 0.6949]])\n",
      "tensor([-0.6088])\n"
     ]
    }
   ],
   "source": [
    "print(W_target)\n",
    "print(b_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, define a number of functions to generate the input (variables) and output (target/response). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(x):\n",
    "    \"\"\"Builds features i.e. a matrix with columns [x, x^2, x^3, x^4].\"\"\"\n",
    "    x = x.unsqueeze(1)\n",
    "    return torch.cat([x ** i for i in range(1, POLY_DEGREE+1)], 1)\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"Approximated function.\"\"\"\n",
    "    return x.mm(W_target) + b_target.item()\n",
    "\n",
    "def poly_desc(W, b):\n",
    "    \"\"\"Creates a string description of a polynomial.\"\"\"\n",
    "    result = 'y = '\n",
    "    for i, w in enumerate(W):\n",
    "        result += '{:+.2f} x^{} '.format(w, len(W) - i)\n",
    "    result += '{:+.2f}'.format(b[0])\n",
    "    return result\n",
    "\n",
    "def get_batch(batch_size=32):\n",
    "    \"\"\"Builds a batch i.e. (x, f(x)) pair.\"\"\"\n",
    "    random = torch.randn(batch_size)\n",
    "    x = make_features(random)\n",
    "    y = f(x)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a simple(st) neural network, which is a **single fully connected** (FC) layer. See [`torch.nn.Linear`](https://pytorch.org/docs/master/nn.html#torch.nn.Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=4, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "fc = torch.nn.Linear(W_target.size(0), 1)\n",
    "print(fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This is a *network* with four input units, one output unit, with a bias term.\n",
    "    \n",
    "* Now generate the data. Let us try to get five pairs of (x,y) first to inspect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6611,  0.4371, -0.2890,  0.1910],\n",
      "        [ 2.5225,  6.3631, 16.0509, 40.4884],\n",
      "        [ 0.5631,  0.3170,  0.1785,  0.1005],\n",
      "        [ 0.6042,  0.3650,  0.2205,  0.1333],\n",
      "        [ 0.4876,  0.2377,  0.1159,  0.0565]])\n",
      "tensor([[ 4.2059],\n",
      "        [59.0951],\n",
      "        [-2.5254],\n",
      "        [-2.5234],\n",
      "        [-2.4667]])\n"
     ]
    }
   ],
   "source": [
    "sample_x, sample_y = get_batch(5)\n",
    "print(sample_x)\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Take a look at the FC layer weights (randomly initialised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2693, -0.4249, -0.2078,  0.3418]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(fc.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Reset the gradients to zero, perform a forward pass to get prediction, and compute the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.069257736206055\n"
     ]
    }
   ],
   "source": [
    "fc.zero_grad()\n",
    "output = F.smooth_l1_loss(fc(sample_x), sample_y)\n",
    "loss = output.item()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Random did not give a good prediction. Let us do a backpropagation and update model parameters with gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.backward() \n",
    "for param in fc.parameters():  \n",
    "    param.data.add_(-0.1 * param.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Check the updated weights and respective loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2734, -0.3073,  0.0971,  1.1496]], requires_grad=True)\n",
      "4.4717488288879395\n"
     ]
    }
   ],
   "source": [
    "print(fc.weight)\n",
    "output = F.smooth_l1_loss(fc(sample_x), sample_y)\n",
    "loss = output.item()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * Now keep feeding more data until the loss is small enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "for batch_idx in count(1):\n",
    "    # Get data\n",
    "    batch_x, batch_y = get_batch()\n",
    "\n",
    "    # Reset gradients\n",
    "    fc.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    output = F.smooth_l1_loss(fc(batch_x), batch_y)\n",
    "    loss = output.item()\n",
    "\n",
    "    # Backward pass\n",
    "    output.backward()\n",
    "\n",
    "    # Apply gradients\n",
    "    for param in fc.parameters():\n",
    "        param.data.add_(-0.1 * param.grad.data)\n",
    "\n",
    "    # Stop criterion\n",
    "    if loss < 1e-3:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.000478 after 359 batches\n",
      "==> Learned function:\ty = -5.77 x^4 +3.06 x^3 +1.66 x^2 +0.69 x^1 -0.59\n",
      "==> Actual function:\ty = -5.78 x^4 +3.07 x^3 +1.66 x^2 +0.69 x^1 -0.61\n"
     ]
    }
   ],
   "source": [
    "print('Loss: {:.6f} after {} batches'.format(loss, batch_idx))\n",
    "print('==> Learned function:\\t' + poly_desc(fc.weight.view(-1), fc.bias))\n",
    "print('==> Actual function:\\t' + poly_desc(W_target.view(-1), b_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B3. Exercises\n",
    "\n",
    "\n",
    "* Change the [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions) to different choices and compare the results.\n",
    "  \n",
    "* Formulate another regression problem and solve it using `torch.nn`\n",
    "* Compare the `torch.nn` solution against the closed-form solution\n",
    "* Explore any other variations that you can think of to learn more\n",
    "\n",
    "\n",
    "# Acknowledgement\n",
    "Some part of this notebook is taken from the following sources\n",
    "\n",
    "* [PyTorch tutorial from CSE446, University of Washington](https://courses.cs.washington.edu/courses/cse446/18wi/sections/section7/446_pytorch_tutorial.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
